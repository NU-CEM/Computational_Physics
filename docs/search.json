[
  {
    "objectID": "notebooks/runge_kutta_method.html",
    "href": "notebooks/runge_kutta_method.html",
    "title": "Runge Kutta",
    "section": "",
    "text": "Questions\n\n\n\n\nHow do I use the Runge-Kutta method for more accurate solutions?\nWhat do the terms first-order accurate and second-order accurate mean?\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\nUse the Runge-Kutta method, implemented in Python, to solve a first-order ODE\nCompare results at different levels of approximation using the matplotlib library.\n\n\n\n\nThe Runge-Kutta method is more accurate than Euler’s method and runs just as fast\nSo far we have used Euler’s method for solving ODEs. We have learnt that, using this method, the final expression for the total error is linear in \\(h\\). However for roughly the same compute time we can reduce the total error so it is of order \\(h^2\\) by implementing another method - the Runge-Kutta method.\nIt is common to use the Runge-Kutta method for solving ODEs given the improved accuracy over Euler’s method. However Euler’s method is still commonly used for PDEs (where there are other, larger, sources of error).\n\nNote: The Runge-Kutta method is actually a family of methods. In fact, Euler’s method is the first-order Runge-Kutta method. There is then the second-order Runge-Kutta method, third-order Runge-Kutta method, and so on..\n\nEuler’s method does not take into account the curvature of the solution, whilst Runge-Kutta methods do, by calculating the gradient at intermediate points in the (time-)step. For example, in the image below we see two estimates to the blue line. Euler’s method is equivalent to the orange line, where we have taken the slope at time \\(t\\) and extrapolated it to \\(t+h\\). A Runge-Kutta type method performs extrapolation using the slope (or slopes) at an intermediate time (or multiple intermediate times). In this case, the green line formed from the slope at \\(t+\\frac{h}2\\) gives a better approximation at \\(t+h\\). This green line is a visual representation of the second-order Runge Kutta method, which is also known as the “midpoint method”.\n\n\n\nRunge-Kutta methods are derived from Taylor expansion(s) around intermediate point(s)\nTo derive the second-order Runge-Kutta method we:\n\nestimate \\(x(t+h)\\) using a Taylor expansion around \\(t+\\frac{h}{2}\\):\n\n\\[\\begin{equation}\nx(t+h) = x(t+\\frac{h}{2}) + \\frac{h}{2}\\left(\\frac{\\mathrm{d}x}{\\mathrm{d}t}\\right)_{t+\\frac{h}{2}} + \\frac{h^2}{8}\\left(\\frac{\\mathrm{d}^2x}{\\mathrm{d}t^2}\\right)_{t+\\frac{h}{2}}+\\mathcal{O}(h^3)\n\\end{equation}\\]\n\nestimate \\(x(t)\\) using a Taylor expansion around \\(t-\\frac{h}{2}\\):\n\n\\[\\begin{equation}\nx(t) = x(t+\\frac{h}{2}) - \\frac{h}{2}\\left(\\frac{\\mathrm{d}x}{\\mathrm{d}t}\\right)_{t+\\frac{h}{2}} + \\frac{h^2}{8}\\left(\\frac{\\mathrm{d}^2x}{\\mathrm{d}t^2}\\right)_{t+\\frac{h}{2}}+\\mathcal{O}(h^3)\n\\end{equation}\\]\n\nSubtract Equation 2 from Equation 1 and re-arrange:\n\n\\[\\begin{eqnarray}\nx(t+h) &=& x(t) + h\\left(\\frac{\\mathrm{d}x}{\\mathrm{d}t}\\right)_{t+\\frac{h}{2}}+\\mathcal{O}(h^3) \\\\\nx(t+h) &=& x(t) + hf(x(t+\\frac{h}{2}),t+\\frac{h}{2})+\\mathcal{O}(h^3)\n\\end{eqnarray}\\]\nNote that the \\(h^2\\) term has completely disappeared, and the error term is now order \\(h^3\\). We can say that this approximation is now accurate to order \\(h^2\\).\nThe problem is that this requires knowledge of \\(x(t+\\frac{h}{2})\\) which we don’t currently have. We can however estimate this using the Euler method!\n\\[\\begin{equation}\nx(t+\\frac{h}{2}) = x(t) + \\frac{h}{2}f(x,t).\n\\end{equation}\\]\nSubstituting this into Equation 3 above, we can write the method for a single step as follows:\n\\[\\begin{eqnarray}\nk_1 &=& hf(x,t) \\\\\nk_2 &=& hf(x+\\frac{k_1}{2},t+\\frac{h}{2})\\\\\nx(t+h) &=& x(t) + k_2\n\\end{eqnarray}\\]\nSee how \\(k_1\\) is used to give an estimate for \\(x(t+\\frac{h}{2})\\) in \\(k_2\\), which is then substituted into the third equation to give an estimate for \\(x(t+h)\\).\n\n\n\n\n\n\nNote\n\n\n\nHigher orde Runge-Kutta methods can be derived in a similar way - by calculating the Taylor series around various points and then taking a linear combination of the resulting expansions. As we increase the number of intermediate points, we increase the accuracy of the method. The downside is that the equations get increasingly complicated.\n\n\n### Runge-Kutta methods can be applied using the Python skills we have developed\nTo demonstrate the Runge-Kutta method with a simple example, we will re-visit the differential equation for nuclear decay. We will model the decay process over a period of 10 seconds, with the decay constant \\(\\lambda=0.1\\) and the initial condition \\(N_0 = 1000\\):\n\\[\\begin{equation}\n\\frac{\\mathrm{d}N}{\\mathrm{d} t} = -0.1 N\n\\end{equation}\\]\nFirst, let’s import the standard scientific libraries we will be using - Numpy and Matplotlib:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nLet’s definte the function \\(f(N,t)\\) which describes the rate of decay. In this case, the function depends only on the number of atoms present.\n\n# define the function for nuclear decay\ndef f(Num_atoms):\n    return -0.1*Num_atoms\n\nNext we’ll list the simulation parameters: start time, end time, number of time steps and step size (which is calculated using the number of time steps). For comparison, these will be the same as we used for Euler’s method.\n\na = 0                  # start time\nb = 10                 # end time\nnum_steps = 5         # number of time steps\nh = (b-a) / num_steps  # time step size\n\nWe then specify the initial conditions - which in this case is the number of atoms to begin with:\n\nNum_atoms = 1000       # initial condition\n\nWe use the Numpy arange function to generate a list of evenly spaced times at which to evaluate the number of atoms. We also create an empty list to hold the values for \\(N\\) over time.\n\n# use the Numpy arange function to generate a list of evenly spaced times at which to evaluate the number of atoms N.\ntime_list = np.arange(a,b,h)\n\n# create an empty list to hold the calculated N values\nNum_atoms_list = []\n\nFinally, we apply second-order Runge-Kutta method using a For loop.\n\n# apply Runge-Kutta method. \nfor time in time_list:\n    Num_atoms_list.append(Num_atoms)\n    k1 = h*f(Num_atoms)\n    k2 = h*f(Num_atoms+0.5*k1)\n    Num_atoms += k2\n\n\n\nWe can easily compare our various models using the matplotlib plotting library\nUsing the analytic solution from a previous lesson, we can define a function for calculating the number of atoms \\(N\\) as a function of time (this is the exact solution).\n\ndef analytic_solution(time):\n    return 1000*np.exp(-0.1*time)\n\nWe can use this to calculate the exact value for \\(N\\) over the full time range. We use a large number of points in time (in this case 1000) to give a nice smooth curve - note that we have renamed the variables for the analytic case so we do not override the original calculation parameters.\n\nnum_steps_analytic = 1000\nh_analytic = (b-a) / num_steps_analytic\ntime_analytic_list = np.arange(a,b,h_analytic)\nNum_atoms_analytic_list = []\n\nfor time in time_analytic_list:\n    Num_atoms_analytic_list.append(analytic_solution(time))\n\nWe can also re-calculate the atom population using Euler’s method for comparison to the Runge-Kutta method:\n\nNum_atoms = 1000\nNum_atoms_euler_list = []\n\nfor time in time_list:\n    Num_atoms_euler_list.append(Num_atoms)\n    Num_atoms += h*f(Num_atoms)\n\nFinally we plot all three models side-by-side:\n\nplt.plot(time_analytic_list,Num_atoms_analytic_list,label=\"analytic\")\nplt.scatter(time_list, Num_atoms_list,label=\"Runge-Kutta (second-order)\")\nplt.scatter(time_list, Num_atoms_euler_list,label=\"Euler's method\")\nplt.xlabel(\"time\")\nplt.ylabel(\"Number of atoms\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x12a014e20&gt;\n\n\n\n\n\n\n\nThe second-order Runge-Kutta method is accurate to order \\(h^2\\)\n\nWe have seen earlier in the course that Euler’s method is a first-order method accurate to order \\(h\\).\nThe error term for one step of the Runge-Kutta method is \\({O}(h^3)\\) - this makes the Runge-Kutta method accurate to order \\(h^2\\) which is why this is called the second-order Runge Kutta method (RK2).\nWith the RK2 can use a fewer number of steps whilst getting the same accuracy as Euler’s method.\nThere are higher order Runge-Kutta methods which increase the accuracy further.\n\n\n\n\n\n\n\nKeypoints\n\n\n\n\nThe Runge-Kutta method is more accurate than Euler’s method and runs just as fast\nRunge-Kutta methods are derived from Taylor expansion(s) around intermediate point(s)\nRunge-Kutta methods can be applied using the Python skills we have developed\nWe can easily compare our various models using the matplotlib plotting library\nThe second-order Runge-Kutta method is accurate to order \\(h^2\\)"
  },
  {
    "objectID": "notebooks/FTCS_method.html",
    "href": "notebooks/FTCS_method.html",
    "title": "Forward Time Central Space",
    "section": "",
    "text": "Questions\n\n\n\n\nHow do I use the Forward-Time Centred-Space method (FTCS) to solve the diffusion equation?\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\nApply the Forward-Time Centred-Space method (FTCS) to solve theat heat diffusion equation\n\n\n\n\nThe diffusion equation is an initial value problem\nIn the previous section we solved a boundary value problem in the form of Laplace’s equation. In this section we will look at an initial value problem, which is a more complex type of PDE. An initial value problem is more complex as we are told the starting conditions and then have to predict future behaviour as a function of time.\nThe example we will use is the one-dimensional diffusion equation:\n\\[\\begin{equation}\n\\frac{\\partial \\phi}{\\partial t} = D\\frac{\\partial^2 \\phi}{\\partial x^2}\n\\end{equation}\\]\nIn this case we have a variable \\(\\phi(x,t)\\) that depends on position \\(x\\) and time \\(t\\) - so can we not solve it in the same way as finding the \\(\\phi(x,y)\\) Laplace’s equation, which also had two independent variables?\nThe problem is that we only have an initial condition in the time dimension - we know the value of \\(\\phi(x,t)\\) at \\(t=0\\) but we do not typically know the value of \\(t\\) at a later point. In the spatial dimensions we know the boundary conditions at either end of the grid.\nInstead, we will use the Forward-Time Centred-Space method (FTCS).\n\n\nThere are two steps to the Forward-Time Centred-Space method\n\nStep one\nUse the finite difference method to express the 1D Laplacian as a set of simulatenous equations:\n\\[\\begin{equation}\n\\frac{\\partial^2\\phi}{\\partial x^2} = \\frac{\\phi(x+a,t)+\\phi(x-a,t) - 2\\phi(x,t)}{a^2}\n\\end{equation}\\]\nwhere \\(a\\) is the grid spacing.\nSubstitute this back into the diffusion equation to give a set of simulataneous ODEs:\n\\[\\begin{equation}\n\\frac{d \\phi}{d t} = \\frac{D}{a^2}(\\phi(x+a,t)+\\phi(x-a,t)-2\\phi(x,t))\n\\end{equation}\\]\nIf there are \\(N\\) grid points then Equation 3 corresponds to a set of \\(N\\) equations. It is an ODE as there is derivative with respect to only one variable - time.\n\n\nStep two\nWe now have a set of simultaneous ODEs for \\(\\phi(x,t)\\). So we can use Euler’s method to evolve the system forward in time. Euler’s method for solving an ODE of the form \\(\\frac{d\\phi}{dt} = f(\\phi,t)\\) has the general form:\n\\[\\begin{equation}\n\\phi(t+h) \\simeq \\phi(t) + hf(\\phi, t).\n\\end{equation}\\]\nApplying this to Equation 3 gives:\n\\[\\begin{equation}\n\\phi(x,t+h) = \\phi(x,t) + h\\frac{D}{a^2}(\\phi(x+a,t)+\\phi(x-a,t)-2\\phi(x,t))\n\\end{equation}\\]\n\n\n\nThe FTCS method can be applied using the Python skills we have developed\nConsider a 10cm rod of stainless steel initially at a uniform temperature of 20\\(^\\mathrm{o}\\) Celsius. The rod is dipped in a hot water bath at 90\\(^\\mathrm{o}\\) Celsius at one end, and held in someone’s hand at the other. Assume that the hand is at constant body temperature throughout (27\\(^\\mathrm{o}\\) Celsius).\nThe problem can be represented visually as follows:\n\n\n\nOur goal is to calculate the temperature profile of the steel as a function of distance \\(x\\) from the cold side to the hot side, and as a function of time. For simplicity let us assume that the rod is perfectly insulated so that heat only moves horizontally; as a result this problem can be modelled as 1-dimensional. Also assume that neighter the hot water bath or the hand change temperature appreciably.\nThermal conduction is described by the diffusion equation (or heat equation in this context)\n\\[\\begin{equation}\n\\frac{\\partial \\phi}{\\partial t} = D\\frac{\\partial^2 \\phi}{\\partial x^2},\n\\end{equation}\\]\nwhere \\(D\\) is the material dependent thermal diffusivity. For steel \\(D=4.25\\times10^{-6}\\mathrm{m}^2\\mathrm{s}^{-1}\\).\nFirst, let’s import the libraries we will be using\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nNow let’s fix some of the constants in the problem\n\nL = 0.1 # width of the rod in metres\nD = 4.25e-6 # thermal diffusivity of steel\n\nAnd some of the numerical parameters:\n\nN = 100 # number of divisions in the grid\na = L/N # grid spacing\nh = 1e-4 # time step\n\nLet’s specify the boundary conditions and the initial condition\n\nT_left = 27.0  # temperature fixed on left side of rod\nT_right = 90.0 # temperature fixed on right side of rod\nT_middle = 20.0 # temperature of rod at the beginning\n\nAnd now create an array \\(T\\) to hold the temperature of the rod and an array \\(T_{\\mathrm{new}}\\) to calculate the temperature of the rod after evolving through time. Note that there are \\(N\\) grid divisions but \\(N+1\\) grid points (as we are evaluating at the boundary on each edge).\n\nT = np.empty(N+1, float)  \nT_new = np.empty(N+1, float)\n\nApply the boundary conditions and initial condition to our rod:\n\nT[0] = T_left\nT[-1] = T_right\nT[1:-1] = T_middle\nT_new[0] = T_left\nT_new[-1] = T_right\n\n\ndef evolve(T,T_new):\n    c = (h*D)/(a*a)\n    T_new[1:N] = T[1:N] + c*(T[2:N+1]+T[0:N-1]-2*T[1:N])\n    return T, T_new\n    \n\nNote that in the function evolve we implement Equation 5. We make use of Numpy’s element-by-element array manipulation to evaluate the \\(N-1\\) equations in a single step.\n\nepsilon = h/100\n\ntimes=np.array([0.01,0.1,1,10,100])\nt_end = times[-1]+epsilon\nt=0.0\n\n\nwhile t &lt; t_end:\n    \n    T_new, T = evolve(T, T_new)\n    t+=h\n    \n    for time in times:\n        if abs(t-time)&lt;epsilon:\n            plt.plot(T,label=\"{}s\".format(round(t,ndigits=3)))\n    \nplt.legend()\nplt.title(\"Heat diffusion along a 1D rod\")\nplt.xlabel(\"Position (cm)\")\nplt.ylabel(\"Temperature (degrees centigrade)\")\n\nText(0, 0.5, 'Temperature (degrees centigrade)')\n\n\n\n\n\n\n\n\n\n\n\nKeypoints\n\n\n\n\nThe diffusion equation is an initial value problem\nThere are two steps to the Forward-Time Centred-Space (FTCS) method\nThe FTCS method can be applied using the Python skills we have developed"
  },
  {
    "objectID": "notebooks/integration.html",
    "href": "notebooks/integration.html",
    "title": "Riemann sums",
    "section": "",
    "text": "Questions\n\n\n\n\nHow can I do basic numerical integration?\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\nUse the rectangular-slice approximation to calculate integrals\nDescribe the difference between a zeroth-order and first-order integration rule\n\n\n\n\nDepending on the functional form of f(x), it may not be possible to calculate an integral analytically\nThe integral of \\(f(x)\\) from \\(a\\) to \\(b\\) is the area under the curve. Depending on the functional form of \\(f(x)\\), it may not be possible to calculate the integral analytically.\n\n\n\n\n\nRiemann sums are a family of methods used for approximating integrals\nThe Riemann definition of the integral \\(I\\) is:\n\\[ I = \\lim_{n\\to\\infty} \\sum_{i=1}^N h f(x_i)\\]\nwhere the domain of integration has been split into \\(N\\) slices, each with width \\(h = \\frac{b-a}{N}\\). As we cannot in practice consider an infinite number of slices, this definition will give an approximation to the exact answer. By making \\(N\\) large are approximation will, in many cases, be reasonable.\n\n\nThe simplest Riemann sum is based on rectangular slices\nThe simplest way to use this approach is to calculate \\(f(x_i)\\) at some point on each slice and then calculate the area of the associated rectangle:\n\\[ A_i = hf(x_i)\\]\nThe integral is given by summing over all of the rectangles:\n\\[ \\int_a^b f(x_i) dx \\approx \\sum_{i=1}^N A_i \\]\n\n\n\n\n\nThe rectangular slices method can be translated to Python code in a straight-forward manner\nFor example, we may want to calculate the integral of \\(\\sin(x)\\) from 0 to \\(\\frac{\\pi}{2}\\). This is an integral that can be evaluated analytically, so it doesn’t usually make sense to calculate numerically - however, in this case, we can use it to establish that our method is correct.\n\nimport math\n\ndef sin(x):\n    \n    return math.sin(x)\n\ndef rectangular_slice_integral(f_x, a, b, N):\n    \n    integral = 0\n    h = (b-a) / N   # h is the width of each slice\n    for i in range(N):\n        x = a + h*i # the x value for the slice\n        integral += f_x(x)*h\n    return integral\n\nNote that the function rectangular_slice_integral has an argument f_x which is itself a function. This is valid Python - you can pass one function to another function as an argument.\n\nrectangular_slice_integral(sin,0,math.pi/2,100)\n\n0.9921254566056334\n\n\nIn fact, it is possible to pass the math.sin() function directly to rectangular_slice_integral():\n\nrectangular_slice_integral(math.sin,0,math.pi/2,100)\n\n0.9921254566056334\n\n\nThis is pretty close to the correct value of 1. To improve our approximation we can increase the number of slices:\n\nrectangular_slice_integral(math.sin,0,math.pi/2,200)\n\n0.9960678687587687\n\n\n The Riemann sums method a zeroth-order integration rule that will integrate a zeroth-order polynomial (ie, constant number) exactly. It has an error of order \\(h\\) (\\(\\mathcal{O}(h)\\)) - when we halve the rectangular width, we halve the error. \n\n\nRiemann sums can be adapted for use with discrete data\nNot all integrations are integrations of functions. For example, we may want to integrate experimental data, in which case there is no function to call to find the value of f(x). Instead, the most likely form of f(x) is given by the list of data values. In this case we can use the same method, but the implementation is slightly different:\n\ndef rectangular_slice_integral_discrete(data, h):\n    \n    return h*sum(data)\n\n Note that this assumes the data is evenly spaced at width \\(h\\) .\nTo test our function using the same example as above we need to generate a list of sin(x) values between 0 to \\(\\frac{\\pi}{2}\\):\n\nimport numpy as np\n\nh = (math.pi/2)/100\nsin_0_90 = [math.sin(x) for x in np.arange(0,math.pi/2,h)]\n\nwhere we are using Python list comprehension and the Numpy arange function to generate a list of evenly spaced floats.\nIf we are simulating experimental data we should add a little noise or randomness to the data. We can use the Python standard library random and list comprehension to do this:\n\nimport random\n\nsin_0_90_noise = [x+random.uniform(-0.1,0.1) for x in sin_0_90]\n\nWe can now pass this list to our function rectangular_slice_integral_discrete:\n\nrectangular_slice_integral_discrete(sin_0_90_noise, h)\n\n1.0094729791206596\n\n\nWe can visualise the exact sinusoidal curve and noisy sinusoidal curve using the matplotlib plotting library:\n\nimport matplotlib.pyplot as plt\n\nplt.plot(np.arange(0,math.pi/2,h),sin_0_90,label=\"exact sine\")\nplt.plot(np.arange(0,math.pi/2,h),sin_0_90_noise,label=\"noisy sine\")\n\n\n\n\n\n\nHigher-order Riemann sums increase the accuracy of our approximations\nWe can greatly improve the efficiency of our integration by approximating the slices as trapezoids instead of as rectangles. This is because the area under the trapezoids is a considerably better approximation to the area under the curve.\n The trapezoidal rule a first-order integration rule that will integrate a first-order polynomial (ie, a straight line) exactly. We can say it is accurate to order \\(h\\) (\\(\\mathcal{O}(h)\\)) and has an error of order \\(h^2\\) \\(\\mathcal{O}(h^2)\\) .\n\n\n\nIn many cases we can use Simpson’s Rule for greater accuracy still. This technique involves fitting quadratic curves to pairs of slices and then calculating the area under the quadratics. In many cases Simpson’s rule is more accurate than the trapezoidal rule, but this is not guaranteed for all integrands.\n\n\n\n\n\n\nKeypoints\n\n\n\n\nDepending on the functional form of f(x), it may not be possible to calculate an integral analytically\nRiemann sums are a family of methods used for approximating integral\nThe simplest Riemann sum is based on rectangular slices\nThe rectangular slices method can be translated to Python code in a straight-forward manner\nRiemann sums can be adapted for use with discrete data\nHigher-order Riemann sums increase the accuracy of our approximations\n\n\n\n\n\nTest your understanding\n\n\n\n\n\n\nIntegrating a semicircle\n\n\n\n\n\n\nUse Riemann sums (with 10 rectangular slices) to calculate the value of the integral:\n\n\\[ I = \\int_{-1}^1\\sqrt{1-x^2}\\mathrm{d}x \\]\n\nHow does this compare to exact answer? (Hint: the integrand is a semicircle of radius 1)\nHow can you improve the accuracy of your estimate?\n\n\n\n\n\n\n\nShow answer\n\n\n\n\n\n\nWe can use the same approach as in the tutorial, but with a different function for calculating the integrand.\n\nimport math\n\ndef semicircle(x):\n    \n    return math.sqrt(1-x**2)\n\ndef rectangular_slice_integral(f_x, a, b, N):\n    \n    integral = 0\n    h = (b-a) / N   # h is the width of each slice\n    for i in range(N):\n        x = a + h*i # the x value for the slice\n        integral += f_x(x)*h\n    return integral\n    \nrectangular_slice_integral(semicircle, -1, 1, 100)\n1.5691342555492505\n\nThe exact answer is \\(\\frac{\\pi}{2}\\). The error on our calculation is\n\nmath.pi/2 - rectangular_slice_integral(semicircle, -1, 1, 100)\n0.0016620712456461018\n\nTo improve the accuracy we can use a larger number of slices:\n\nmath.pi/2 - rectangular_slice_integral(semicircle, -1, 1, 1000)\n5.2588293825595045e-05"
  },
  {
    "objectID": "notebooks/PDE_boundary_and_initial.html",
    "href": "notebooks/PDE_boundary_and_initial.html",
    "title": "Initial and boundary conditions",
    "section": "",
    "text": "Questions\n\n\n\n\nWhen do I need boundary conditions and initial conditions?\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\nIdentify boundary value problems and initial value problems\n\n\n\n\nPDEs can have boundary conditions and initial conditions\nIn the previous section of the course we learnt that ODEs have either initial values or boundary values.\n\nBoundary value problems\nPDEs can also be separated in a similar manner. Boundary value problems describe the behaviour of a variable in a space and we are given some constraints on the variable around the boundary of that space. For example, consider the 2-dimensional problem of a thin rectangular sheet with one side at voltage \\(V\\) and all others at voltage zero.\n\n\n\nThe specification that one side is at voltage \\(V\\) and all others are at voltage zero are the boundary conditions. We could then calculate the electrostatic potential \\(\\phi\\) at all points within the sheet using the two-dimensional Laplace’s equation:\n\\[\\begin{equation}\n\\nabla^2\\phi = \\frac{\\partial^2\\phi}{\\partial x^2} + \\frac{\\partial^2\\phi}{\\partial y^2} = 0\n\\end{equation}\\]\n\n\nInitial value problems\nInitial value problems are where the field - or other variable of interest - is varying in both space and time. We now require boundary conditions and initial values. This is a more difficult type of PDE to solve.\nFor example, consider heat diffusion in a two-dimensional sheet. Here we could specify that there is no heat flow in or out of the sheet - this is the boundary condition.\n\n\n\nWe could also specify that at time \\(t=0\\) the centre of the sheet is at temperature \\(T_1\\), whilst surrounding areas are at temperature \\(T_0\\). This is the initial condition. It differs from a boundary condition in that we are told what the temperature is at the start of our time grid (at \\(t=0\\)) but not at the end of our time grid (when the simulation finishes).\n\n\n\nWe could then calculate the temperature at time \\(t\\) at all points \\([x,y]\\) within the sheet using the two-dimensional Diffusion equation:\n\\[\\begin{equation}\n\\nabla^2T = \\frac{\\partial^2 T}{\\partial x^2} + \\frac{\\partial^2 T}{\\partial y^2}=\\alpha \\frac{\\partial T}{\\partial t}\n\\end{equation}\\]\n\n\n\n\n\n\nKeypoints\n\n\n\n\nPDEs can have boundary values and initial values"
  },
  {
    "objectID": "notebooks/finite_difference.html",
    "href": "notebooks/finite_difference.html",
    "title": "Finite difference methods",
    "section": "",
    "text": "Questions\n\n\n\n\nHow do I use a finite difference method to calculate derivatives?\nWhat is the Laplacian operator?\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\nUse the finite difference method to calculate the derivative of an unknown function\nExpress the Laplacian as a differential operator\n\n\n\n\nFinite difference methods are a family of techniques used to calculate derivatives\nFinite-difference methods are a class of numerical techniques for solving differential equations by approximating derivatives with finite differences. They are widely used for solving ordinary and partial differential equations, as they can convert equations that are unsolvable analytically into a set of linear equations that can be solved on a computer.\nThey rely on the idea of discretization: breaking a domain (for example, the space domain) into a finite number of discrete elements.\n\n\n\nA numerical derivative can be calculated using the forward, backward or central difference methods\n\nThe standard definition of a derivative is\n\\[\\begin{equation}\n\\frac{\\mathrm{d} f}{\\mathrm{d} x} = \\lim_{h\\to0}\\frac{f(x+h)-f(x)}{h}.\n\\end{equation}\\]\nTo calculate the derivative numerically we make \\(h\\) very small and calculate\n\\[\\begin{equation}\n\\frac{\\mathrm{d} f}{\\mathrm{d} x} \\simeq \\frac{f(x+h)-f(x)}{h}.\n\\end{equation}\\]\nThis is the forward difference because it is measured in the forward direction from \\(x\\).\nThe backward difference is measured in the backward direction from \\(x\\):\n\\[\\begin{equation}\n\\frac{\\mathrm{d} f}{\\mathrm{d} x} \\simeq \\frac{f(x)-f(x-h)}{h},\n\\end{equation}\\]\nand the central difference uses both the forwards and backwards directions around \\(x\\):\n\\[\\begin{equation}\n\\frac{\\mathrm{d} f}{\\mathrm{d} x} \\simeq \\frac{f(x+\\frac{h}{h2})-f(x-\\frac{h}{2})}{h}.\n\\end{equation}\\]\nLet’s start with a simple example - let’s use the forward difference method to calculate the derivative of \\(x^2\\) at \\(x=5\\) with \\(h=0.01\\).\n\ndef x_squared(x):\n    return 2*x**2\n\ndef forward_difference(f_x, x, h):\n    return (f_x(x+h) - f_x(x)) / h\n    \n\n\nforward_difference(x_squared,5, 0.01)\n\n20.019999999999527\n\n\n\n\nWe need to converge with respect to the step size \\(h\\)\nOur expressions above are approximations as they are only exactly equal to the derivative when the step size \\(h\\) is zero. Whether using forwards, backwards or central differences it is important to converge with respect to a decreasing step size \\(h\\).\nNote that in the next tutorial we will see that it is also possible to make \\(h\\) too small!\nAs we can see from the example in the image at the top of the page, the central difference is (in general) more accurate than the forward or backward differences.  In fact, the error is order \\(h\\) for the forwards and backwards methods, and order \\(h^2\\) for the central difference. \nLet’s test this idea using our simple \\(2x^2\\) example that we started above:\n\nexact_answer = 20\n\ndef calculate_x2_error(h):\n    error = exact_answer - forward_difference(x_squared,5, h)\n    print (\"error for h={} is {}\".format(h,round(error,10)))\n\n\ncalculate_x2_error(0.01)\ncalculate_x2_error(0.005)\ncalculate_x2_error(0.0025)\n\nerror for h=0.01 is -0.02\nerror for h=0.005 is -0.01\nerror for h=0.0025 is -0.005\n\n\nWe can see that as the step size \\(h\\) is halved, the error halves.\n\n\nSecond-order derivatives can be calculated using finite differences\nThe second derivative is a derivative of a derivative, and so we can calculate it be applying the first derivative formulas twice. The resulting expression (after application of central differences) is:\n\\[\\begin{equation}\n\\frac{\\mathrm{d}^2f}{\\mathrm{d} x^2} \\simeq \\frac{f(x+h)-2f(x)+f(x-h)}{h^2}.\n\\end{equation}\\]\nLet’s test this out using the \\(2x^2\\) example that we started above:\n\ndef second_order_forward_difference(f_x, x, h):\n    return (f_x(x+h) - 2*f_x(x) + f_x(x-h)) / (h**2)\n\nsecond_order_forward_difference(x_squared, 5, 0.01)\nThe second derivative of \\(2x^2\\) is 4, so the implementation appears correct.\n\n\nPartial derivatives can be calculated using finite differences\nThe extension to partial derivatives is also relatively straight-forward. At this point we also consider a second dependent variable, \\(y\\).\n\\[\\begin{equation}\n\\frac{\\partial f}{\\partial x} \\simeq \\frac{f(x+\\frac{h}{2},y)-f(x-\\frac{h}{2},y)}{h},\n\\end{equation}\\]\n\\[\\begin{equation}\n\\frac{\\partial f}{\\partial y} \\simeq \\frac{f(x,y+\\frac{h}{2})-f(x,y-\\frac{h}{2})}{h},\n\\end{equation}\\]\n\\[\\begin{equation}\n\\frac{\\partial ^2f}{\\partial x^2} \\simeq \\frac{f(x+h,y)-2f(x,y)+f(x-h,y)}{h^2},\n\\end{equation}\\]\n\\[\\begin{equation}\n\\frac{\\partial ^2f}{\\partial y^2} \\simeq \\frac{f(x,y+h)-2f(x,y)+f(x,y-h)}{h^2}.\n\\end{equation}\\]\nLet’s consider another example, where we calculate the \\(x\\)-component of a force \\(F\\) in a potential energy \\(U = x^2+y^2\\), at \\(x=5,y=10\\). We know that the force and potential energy are calculated as follows:\n\\[F_x = \\frac{\\partial U}{\\partial x}\\]\n\ndef potential_energy(x,y):\n    return x**2 + y**2\n\ndef partial_dfdx(f_x, x, y, h):\n    return (f_x(x+(h/2),y) - f_x(x-(h/2),y)) / h\n\n\npartial_dfdx(potential_energy, 5, 10, 0.01)\n\n10.000000000000853\n\n\nWhich is close to the analytic answer of 10.\n\n\nThe Laplacian operator corresponds to an average rate of change\nThe Laplacian operator \\(\\nabla^2\\) is a very important differential operator in physics. We will see it later in the course, when studying partial differential equations. It is used to mathematically describe a variety of physical processes, including diffusion, gravitational potentials, wave propogation and fluid flow.\nWhen applied to \\(f\\) and written in full for a two dimensional cartesian coordinate system with dependent variables \\(x\\) and \\(y\\) it takes the following form:\n\\[\\begin{equation}\n\\nabla^2\\phi = \\frac{\\partial^2\\phi}{\\partial x^2} + \\frac{\\partial^2\\phi}{\\partial y^2} + \\frac{\\partial^2\\phi}{\\partial z^2}.\n\\end{equation}\\]\nWith equivalent expressions for a single dimension or extension to higher dimensions.\nWe can think of the laplacian as encoding an average rate of change. To develop an intuition for how the laplacian can be interpreted physically, we need to understand two related operators - div and curl. We will not explore these operators further in this lesson, but a related video is below:\n\nyoutube: https://youtu.be/EW08rD-GFh0\n\n\n\nThe Laplacian can be calculated using finite differences\nBy adding the two equations for second order partial derivatives (Equations 8 and 9), we find that the Laplacian in two dimensions is:\n\\[\\begin{equation}\n\\frac{\\partial ^2f}{\\partial x^2} + \\frac{\\partial ^2f}{\\partial y^2} \\simeq \\frac{f(x+h,y)+f(x-h,y)+f(x,y+h)+f(x,y-h)-4f(x,y)}{h^2},\n\\end{equation}\\]\nThe expression above is known as a five-point stencil as it uses five points to calculate the Laplacian.\n\n\n\n\n\n\nKeypoints\n\n\n\n\nFinite difference methods are a family of techniques used to calculate derivatives\nA numerical derivative can be calculated using the forward, backward or central finite difference\nWe need to converge with respect to the step size \\(h\\)\nSecond-order derivatives, partial derivatives and the Laplacian can also be calculated using finite differences\nThe Laplacian operator corresponds to an average rate of change\n\n\n\n\n\nTest your understanding\n\n\n\n\n\n\nImplementing the Laplacian\n\n\n\n\n\n\nUse a five-point stencil to calculate\n\n\\[\\nabla^2\\phi = \\frac{\\partial^2\\phi}{\\partial x^2} + \\frac{\\partial^2\\phi}{\\partial y^2} + \\frac{\\partial^2\\phi}{\\partial z^2}\\]\nfor \\(\\phi = 6\\cos(x)+7\\sin(y)\\) at \\(x=\\pi\\) and \\(y=\\pi\\), using a step-size of your choice.\n\nCompare to the exact answer.\n\n\n\n\n\n\n\nShow answer\n\n\n\n\n\n\n\n\nimport math\n\ndef integrand(x,y):\n    return 6*math.cos(x) + 7*math.sin(y)\n\ndef laplacian(f_xy, x, y, h):\n    return (f_xy(x+h,y) + f_xy(x-h,y) + f_xy(x,y+h) + f_xy(x,y-h) - 4*(f_xy(x,y))) / (h**2)    \n  \nlaplacian(integrand, math.pi, math.pi, 1E-2)\n5.999950000159515\n\nThis is within 1e-5 to the exact answer of 6."
  },
  {
    "objectID": "notebooks/relaxation_method.html",
    "href": "notebooks/relaxation_method.html",
    "title": "Relaxation method",
    "section": "",
    "text": "Questions\n\n\n\n\nHow do I use the relaxation method to solve Laplace’s equation?\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\nUse the finite difference method to convert Laplace’s equation into a set of linear simultaneous equations\nUse the relaxation method to solve Laplace’s equation\n\n\n\n\nThe method of finite differences is often used to solve partial differential equations\nConsider the two-dimensional Laplace equation for the electric potential \\(\\phi\\) subject to appropriate boundary conditions:\n\\[\\begin{equation}\n\\frac{\\partial^2\\phi}{\\partial x^2} + \\frac{\\partial^2\\phi}{\\partial y^2} = 0\n\\end{equation}\\]\nReal physical problems are in three dimensions, but we can more easily visualise the method of finite differences - and the extension to three dimensions is straight forward.\nThe method of finite differences, which has already been introduced earlier in the course, involves dividing the space into a grid of discrete points \\([x,y]\\) and calculating numerical derivatives or at each of these points.\n\n\n\nIn this case we consider a 2-dimensional sheet with a fixed voltage \\(V\\) at the top side, and all other sides fixed at \\(0V\\).\nAs a quick recap from what was learnt earlier in the course, we can express the Laplacian in two dimensions using finite differences:\n\\[\\begin{equation}\n\\frac{\\partial ^2f}{\\partial x^2} + \\frac{\\partial ^2f}{\\partial y^2} \\simeq \\frac{f(x+h,y)+f(x-h,y)+f(x,y+h)+f(x,y-h)-4f(x,y)}{h^2},\n\\end{equation}\\]\nThe expression above is known as a five-point stencil as it uses five points to calculate the Laplacian.\n\n\nThe finite difference method turns our partial differential equation into a set of linear simulatenous equation\nReturning to our Laplace equation for for the electric potential \\(\\phi\\):\n\\[\\begin{equation}\n\\frac{\\partial^2\\phi}{\\partial x^2} + \\frac{\\partial^2\\phi}{\\partial y^2} = 0\n\\end{equation}\\]\n\n\n\nThe numerical Laplacian can be substituted into the equation above, giving us a set of \\(n\\) simulatenous equations for the \\(n\\) grid points.\n\\[\\begin{equation}\n\\frac{\\phi(x+h,y)+\\phi(x-h,y)+\\phi(x,y+h)+\\phi(x,y-h)-4\\phi(x,y)}{h^2} = 0,\n\\end{equation}\\]\nwhere \\(h\\) is the distance between each grid point.\n\n\nTo solve this set of equations we use the relaxation method\nTo calculate \\(\\phi(x,y)\\) we use the relaxation method, also known as the Jacobi method in the context of the Laplace equation. First we re-arrange the equation above:\n\\[\\begin{equation}\n\\phi(x,y)=\\frac{1}{4}\\left(\\phi(x+h,y)+\\phi(x-h,y)+\\phi(x,y+h)+\\phi(x,y-h)\\right).\n\\end{equation}\\]\n Note that because we set the Laplacian equal to zero in Equation 3 (for this particular example), the \\(h^2\\) term has dropped out of the expression - this might not be the case for other examples. \nThis tells us that \\(\\phi(x,y)\\) is the average of the surrounding grid points, which can be represented visually as:\n\n\n\nSecond, we fix \\(\\phi(x,y)\\) at the boundaries using the boundary conditions. Third, we guess the initial values of the interior \\(\\phi(x,y)\\) points - our guesses do not need to be good, and can be zero.\nFinally we use Equation 4 to calculate new values of \\(\\phi'(x,y)\\) at all points in space. We take these new \\(\\phi'(x,y)\\) values and feed them into Equation 4 again to calculate new values. We repeat this iterative process until the \\(\\phi(x,y)\\) values converge, and that is our solution.\nConvergence can be tested by specifying what the maximum difference should be between iterations. For example, that \\(\\phi'(x,y)-\\phi(x,y)&lt; 1e-5\\) for all grid points.\n\n\nThe relaxation method is limited by the accuracy of the finite difference method\n\nFor solving PDEs we use the finite difference method (as part of the relaxation method).\nEven if we use a very small target accuracy for convergence of the relaxation method, our accuracy will still be limited by the finite differences. Higher-order finite difference methods (such as the 5-point or 7-point methods) can be used here to improve the overrall accuracy of the calculation.\n\n\n\nThe relaxation methods can be applied using the Python skills we have developed\nWe will now use our Python Skillz to solve Laplace’s equation with the boundary conditions outlined above. Let’s also imagine that the sheet is 1m along each side and that we want a grid spacing of 1cm. First let’s import the libraries we will use:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nNow let’s specify our simulation parameters…\n\ngrid_width = 101 # number of grid points for width of simulation\ngrid_height = 101 # number of grid points for height of simulation\ntarget = 1e-6 # target accuracy to complete convergence\n\n…and our boundary conditions:\n\nV_top = 1.0 # top wall voltage, in volts\nV_left = 0.0 # left wall voltage\nV_right = 0.0 # right wall voltage\nV_bottom = 0.0 # bottom wall voltage\n\nNext let’s create a NumPy array to hold our \\(\\phi(x,y)\\) and \\(\\phi'(x,y)\\) values:\n\nphi = np.zeros([grid_height,grid_width], float)\nphi_prime = np.empty([grid_height,grid_width], float)\n\nNow we apply the boundary conditions to our array:\n\nphi[0,:] = V_top\n\nNow we write a function which uses the finite difference method to calculate an updated \\(\\phi'(x,y)\\) .\n\ndef finite_difference(phi):\n\n    for i in range(grid_height): # for each grid point\n        for j in range(grid_width):\n            if i==0 or i==grid_height-1 or j==0 or j==grid_width-1:\n                phi_prime[i,j] = phi[i,j] # if at boundary, keep fixed\n            else: # otherwise apply finite difference\n                phi_prime[i,j] = (phi[i+1,j]+phi[i-1,j]+phi[i,j+1]+phi[i,j-1]) / 4\n    \n    return phi_prime\n\nFinally let’s use the relaxation method. We repeatedly call the function finite_difference until all values of \\(\\phi(x,y)\\) are converged.\n\ndelta = 1.0 # create delta. It can take any value larger than the target accuracy\n\nwhile delta &gt; 1e-6:  # keep running the following code until delta &lt; 1e-6\n    phi_prime = finite_difference(phi)  # calculate phi_prime\n    delta = np.max(np.abs(phi-phi_prime)) # calculate the maximum difference between phi and phi_prime\n    phi,phi_prime = phi_prime,phi # Swap phi and phi-prime, ready for the next iteration\n\nWe can visualise our result using the function matplotlib.pyplot.imshow which displays our data as an image:\n\nplt.imshow(phi_prime)\n\n&lt;matplotlib.image.AxesImage at 0x7ffd66d69e10&gt;\n\n\n\n\n\nThis result makes sense: there is a region of high electric potential around the top side of the sheet, where the voltage is fixed at 1V, and regions of low potential around the other three walls. If we would like a colour bar to indicate the \\(\\phi(x,y)\\) values across the image then we can use the function matplotlib.pyplot.contourf to produce a filled contour plot. Note that this function flips our image (plotting the values held in the array from left to right, top to bottom) so we use numpy.flip to achieve the expected result.\n\nplt.contourf(np.flip(phi_prime),levels=np.linspace(0,1,101))\nplt.colorbar().set_label(\"Electric potential\")\n\n\n\n\n\n\n\n\n\n\nKeypoints\n\n\n\n\nThe finite difference method for numerical derivatives is often used to solve partial differential equations\nThe finite difference method turns our partial differential equation into a set of linear simulatenous equation\nTo solve this set of equations we use the relaxation method\nThe relaxation method is limited by the accuracy of the finite difference method\nThe relaxation and finite difference methods can be applied using the Python skills we have developed\n\n\n\n\n\nTest your understanding\n\n\n\n\n\n\nBoundary conditions\n\n\n\n\n\nIn this tutorial we implement a finite difference method to solve Laplace’s equation. We use fixed boundary conditions, which are an example of Dirichlet-type boundary conditions.\ndef finite_difference(phi):\n\n    for i in range(grid_height): # for each grid point\n        for j in range(grid_width):\n            if i==0 or i==grid_height-1 or j==0 or j==grid_width-1:\n                phi_prime[i,j] = phi[i,j] # if at boundary, keep fixed\n            else: # otherwise apply finite difference\n                phi_prime[i,j] = (phi[i+1,j]+phi[i-1,j]+phi[i,j+1]+phi[i,j-1]) / 4\n    \n    return phi_prime\nAnother form of boundary condition is a Periodic Boundary Condition. PBCs are often chosen for approximating a large (infinite) system by using a small part called a unit cell, and are most famously used for modelling periodic crystals in solid state physics. Mathematically, PBCs can be expressed for \\(f(x,y)\\) on a two dimensional \\(N \\times N\\) grid as:\n\\[\\begin{equation}\nf[N+1,y] = f[0,y],\n\\end{equation}\\]\n\\[\\begin{equation}\nf[x,N+1] = f[x,0],\n\\end{equation}\\]\nWrite a function which calculates phi_prime using a finite difference method with periodic boundary conditions.\n\n\n\n\n\n\nShow answer\n\n\n\n\n\ndef finite_difference(phi):\n\n    for i in range(N): # for each grid point\n        for j in range(N):\n    \n            i1 = i+1\n            j1 = j+1\n            i2 = i-1\n            j2 = j-1\n            \n            if i == N-1:\n                i1 = 0\n            if i == 0:\n                i2 == N-1\n            if j == N-1:\n                j1 = 0\n            if j == 0:\n                j2 == N-1\n\n            phi_prime[i,j] = (phi[i1,j]+phi[i2,j]+phi[i,j1]+phi[i,j2]) / 4\n    \n    return phi_prime"
  },
  {
    "objectID": "notebooks/ODE_classification.html",
    "href": "notebooks/ODE_classification.html",
    "title": "Classification",
    "section": "",
    "text": "Questions\n\n\n\n\nWhat is a differential equation?\nWhat is the difference between an ordinary (ODE) and partial (PDE) differential equation?\nHow do I classify the different types of differential equations?\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\nIdentify the dependent and independent variables in a differential equation\nDistinguish between and ODE and PDE\nIdentify the order of a differential equation\nDistinguish between linear and non-linear equations\nDistinguish between heterogeneous and homogeneous equations\nIdentify a separable equation\n\n\n\n\nA differential equation is an equation that relates one or more functions and their derivatives\n\nThe functions usually represent physical quantities (e.g. \\(\\mathbf{F}\\)))\nThe derivative represents a rate of change (e.g. acceleration)\nThe differential equation represents the relationship between the two.\nFor example, Newton’s second law for \\(n\\) particles of mass \\(m\\):\n\n\\[\\begin{equation}\n\\mathbf{F}(t,\\mathbf{x},\\mathbf{v}) = m\\frac{d\\mathbf{v}}{dt}\n\\end{equation}\\]\n\n\nAn independent variable is… a quantity that varies independently…\n\nAn independent variable does not depend on other variables\nA  dependent variable depends on the independent variable\n\n\\[\\begin{equation}\n\\mathbf{F}(t,\\mathbf{x},\\mathbf{v}) = m\\frac{d\\mathbf{v}}{dt}\n\\end{equation}\\]\n\n\\(t\\) is an independent variable\n\\(x\\) and \\(v\\) are dependent variables\nWriting \\(x = x(t)\\) makes this relationship clear.\n\n\n\nDifferential equations can be classified in a variety of ways\nThere are several ways to describe and classify differential equations. There are standard solution methods for each type, so it is useful to understand the classifications.\n\nOnce you can cook a single piece of spaghetti, you can cook all pieces of spaghetti!\n\n\nAn ODE contains differentials with respect to only one variable\nFor example, the following equations are ODEs:\n\\[\\begin{equation}\n\\frac{d x}{d t} = at\n\\end{equation}\\] \\[\\begin{equation}\n\\frac{d^3 x}{d t^3} + \\frac{x}{t} = b\n\\end{equation}\\]\nAs in each case the differentials are with respect to the single variable \\(t\\).\n\n\nPartial differential equations (PDE) contain differentials with respect to several independent variables.\nAn example of a PDE is:\n\\[\\begin{equation}\n\\frac{\\partial x}{\\partial t} = \\frac{\\partial x}{\\partial y}\n\\end{equation}\\]\nAs there is one differential with respect to \\(t\\) and one differential with respect to \\(y\\).\n\n\n\n\n\n\nNote\n\n\n\nThere is a difference in notation - for ODEs we use \\(d\\) whilst for PDEs we use \\(\\partial\\).\n\n\n\n\nThe order of a differential equation is the highest order of any differential contained in it.\nFor example:\n\\(\\frac{d x}{d t} = at\\) is first order.\n\\(\\frac{d^3 x}{d t^3} + \\frac{x}{t} = b\\) is third order.\n\n\n\n\n\n\nWarning\n\n\n\n\\(\\frac{d^3 x}{d t^3}\\) does not equal \\(\\left(\\frac{d x}{d t}\\right)^3\\)!\n\n\n\n\nLinear equations do not contain higher powers of either the dependent variable or its differentials\nFor example:\n\\(\\frac{d^3 x}{d t^3} = at\\) and $ = $ are linear.\n\\((\\frac{d x}{d t})^3 = at\\) and \\(\\frac{d^3 x}{d t^3} = x^2\\) are non-linear.\nNon-linear equations can be particularly nasty to solve analytically, and so are often tackled numerically.\n\n\nHomogeneous equations do not contain any non-differential terms\nFor example:\n\\(\\frac{\\partial x}{\\partial t} = \\frac{\\partial x}{\\partial y}\\) is a homogeneous equation.\n\\(\\frac{\\partial x}{\\partial t} - \\frac{\\partial x}{\\partial y}=a\\) is a heterogeneous equation (unless \\(a=0\\)!).\n\n\nSeparable equations can be written as a product of two functions of different variables\nA separable first-order one-variable differential equation takes the form\n\\[\\begin{equation}\nf(x)\\frac{d x}{d t} = g(t)\n\\end{equation}\\]\nSeparable equations are some of the easiest to solve as we can split the equation into two independent parts with fewer variables, and solve each in turn - we will see an example of this in the next lesson.\n\n\n\n\n\n\nKeypoints\n\n\n\n\nAn independent variable is a quantity that varies independently\nDifferential equations can be classified in a variety of ways\nAn ODE contains differentials with respect to only one variable\nThe order is the highest order of any differential contained in it\nLinear equations do not contain higher powers of either the dependent variable or its differentials\nHomogeneous equations do not contain any non-differential terms\n\n\n\n\n\nTest your understanding\n\n\n\n\n\n\nDescribing equations\n\n\n\n\n\nDescribe the following differential equations. Are they - - an ODE or PDE? - first order or higher? - linear or non-linear? - heterogeneous or homogeneous? - separable or non-separable?\n\nThe oscillation of a non-linear driven pendulum,\n\n\\[\\begin{equation}\n\\frac{\\mathrm{d}^2\\theta}{\\mathrm{d}t^2} = -\\frac{g}{l}\\sin(\\theta) + C\\cos(\\theta)\\sin(\\sigma t),\n\\end{equation}\\]\nwhere \\(l\\) and \\(\\sigma\\) are constant parameters and \\(g\\) is the acceleration due to gravity.\n\nThe one dimensional diffusion equation,\n\n\\[\\begin{equation}\n\\frac{\\partial T}{\\partial t} = D\\frac{\\partial ^2 T}{\\partial x^2}.\n\\end{equation}\\]\n\nThe motion of mass \\(m_1\\) in the gravitational field of mass \\(m_2\\) and with a viscous friction term,\n\n\\[\\begin{equation}\nm_1 \\frac{\\mathrm{d}^2 \\mathbf{r}}{\\mathrm{d} t^2} = -\\frac{G m_1 m_2}{r^2} \\mathbf{r} - cv^2\\mathbf{v},\n\\end{equation}\\]\nwhere \\(v\\) is the velocity.\n\n\n\n\n\n\nShow answers\n\n\n\n\n\n\nThis equation is a second order, linear, heterogeneous, non-separable ODE.\nThis equation is a second order, linear, homogeneous PDE. A linear, homogeneous PDE is separable and can be solved using the Separation of Variables.\nThis equation is a second order, non-linear, heterogeneous, non-separable ODE."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website contains resources for the second year course “Theory, computation and experiment” (KD5081) at Northumbria University, UK. The resources have been developed by Dr Lucy Whalley. If you spot any mistakes or have any suggestions please raise an issue on the Github repository for this course.\nThe website is powered by Github, Quarto and Jupyter Notebook. Text is formatted with Markdown and equations are formatted with LaTeX.\nSome of the website content is adapted and derived from several other sources:\n\nThe Setup instructions page is a derivative of materials from Software Carpentry, used under Attribution 4.0 International (CC BY 4.0).\nThe tutorial “Testing and Documentation” is a derivative of the Testing lesson from Code Refinery, used under Attribution 4.0 International (CC BY 4.0).\nSome of the exercises are adapted from the book Computational Physics. These are used with the permission of the book’s author, Mark Newman, and remain under his copyright.\n\nIn addition, parts of the “Getting results” section were inspired by Rudolf Winter’s notes on ODEs and PDEs.\nThe content of this website unless otherwise indicated is licensed under Creative Commons Attribution 4.0 International. You can re-use any original content as long as an attribution is included, and you indicate if changes were made."
  },
  {
    "objectID": "markdown/quality.html",
    "href": "markdown/quality.html",
    "title": "Code Quality",
    "section": "",
    "text": "1. Document your code\n\nInstead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do - Donald Knuth, literate programming\n\nThere are multiple ways you can document your code. Below are three examples:\n\nDocstrings\nDocstrings are the first statement in a module, function, class or method so programmers can understand what it does without having to read the details of the implementation.\nDocstrings are string literals so must be contained within single quote marks (for single line docstrings) or triple quotes (for multiline docstrings). See the example below for a function-level docstring.\n def calc_bulk_density(mass,volume):\n     \"Return dry bulk density = powder mass / powder volume.\"\n     return mass / volume\nDocstrings are preferred over in-line comments (see below) as the docstrings can be easily accessed using the Python help() function. It is also possible to generate online documentation automatically from docstrings.\n\n\nIn-line comments\n# bulk density is the powder mass / powder volume\ndensity = mass / volume \n\n\nMarkdown in a Jupyter Notebook\nFor more extensive discussion you can combine code and text in a single document. See this tutorial for more information about using Markdown in a Jupyter Notebook.\n\n\n\n2. Focus on readability\nYour code should be easily readable by others. This is a big topic! The Pep 8 Style Guide for Python code has further guidance, although it is a daunting document. The most important thing is that you are consistent within your own code.\n\nConsistency is key\nCode formatting (for example, brackets) and use of whitespace should be consistent. For example, do not mix-and-match whitespace as in the code below:\nspam(ham[1], {eggs: 2})   \nspam( ham[ 1 ], { eggs: 2} )\nYou should also avoid mixing data types where possible. For example, using a 2-dimensional Numpy array and a 1-dimensional Numpy array within a simulation would usually be better than using a 2-dimensional Numpy array and a 1-dimensional Python list.\n\n\nVariable and function names\nUse clear, meaningful variable and function names - don’t just use x, p and expect the reader to know what they mean! For example angular_momentum is a better variable name than omega.\n\n\nClear code structure\nImport all of the libraries used at the top of your code. Also define any constants that will not change during your simulation (for example, the radius of the earth) at the top of your code.\nUse Markdown to write section headings in a Jupyter Notebook. You can also use blank lines to split code into logical blocks. Split long lines of your code using a \\ at the end of the line(s). For example:\nprint(\"this is a really really long line of code \\\nthat I'd like split over two lines\")\n\n\n\n3. Avoid duplication\nDuplication of code should be avoided where possible. There are several ways this can be achieved.\n\nWrite functions\nIf you will re-use a block of code multiple times consider encapsulating it in a function. See this tutorial for information about writing functions.\n\n\nUse external libraries\nUse appropriate functions and data-types, including those from external libraries. For example, if you need to perform mathematical operations on an array of values, use Numpy arrays instead of Python lists.\n\n\nUse control structures when appropriate\nUse control structures appropriately. Only use if, while or for loops when necessary.\n\n\n\n4. Think about reproducibility\nWriting reproducible code is difficult. In fact, there are many interesting initiatives designed to improve reproducibility in the computational scientists, such as Reprohacks.\nOne straight-forward thing you can do is print the version number for each package you import using print(packagename.__version__)"
  },
  {
    "objectID": "markdown/PDE_questions.html",
    "href": "markdown/PDE_questions.html",
    "title": "PDE questions",
    "section": "",
    "text": "Adapted from Mark Newman’s book “Computational Physics, p. 412\nIn the tutorial we have used the relaxation method to solve Laplace’s equation for electrostatics:\n\\[\\begin{equation}\n\\nabla^2\\phi = 0\n\\end{equation}\\]\nA more general form of this equation is Poisson’s equation:\n\\[\\begin{equation}\n\\nabla^2\\phi = -\\frac{\\rho}{\\epsilon_0},\n\\end{equation}\\]\nwhich governs the electric potential in the presence of charge density \\(\\rho\\).\nAssume, as in the tutorial example for Laplace’s equation, that there is a 1 metre square. However this time all four walls of the square at fixed at zero volts and that there are two 20cm x 20cm charged areas in the box. One charge has a density +1Cm\\(^{-2}\\), the other has a density -1Cm\\(^{-2}\\).\n\nModify the code for the Laplace’s equation example to solve this problem using the relaxation method."
  },
  {
    "objectID": "markdown/PDE_questions.html#modelling-the-poisson-equation",
    "href": "markdown/PDE_questions.html#modelling-the-poisson-equation",
    "title": "PDE questions",
    "section": "",
    "text": "Adapted from Mark Newman’s book “Computational Physics, p. 412\nIn the tutorial we have used the relaxation method to solve Laplace’s equation for electrostatics:\n\\[\\begin{equation}\n\\nabla^2\\phi = 0\n\\end{equation}\\]\nA more general form of this equation is Poisson’s equation:\n\\[\\begin{equation}\n\\nabla^2\\phi = -\\frac{\\rho}{\\epsilon_0},\n\\end{equation}\\]\nwhich governs the electric potential in the presence of charge density \\(\\rho\\).\nAssume, as in the tutorial example for Laplace’s equation, that there is a 1 metre square. However this time all four walls of the square at fixed at zero volts and that there are two 20cm x 20cm charged areas in the box. One charge has a density +1Cm\\(^{-2}\\), the other has a density -1Cm\\(^{-2}\\).\n\nModify the code for the Laplace’s equation example to solve this problem using the relaxation method."
  },
  {
    "objectID": "markdown/ODE_introduction.html",
    "href": "markdown/ODE_introduction.html",
    "title": "Introduction to Ordinary Differential Equations",
    "section": "",
    "text": "“Since Newton, mankind has come to realise that the laws of physics are always expressed in the language of differential equations” – Steven Strogatz\n\nIn this section of the course we will learn how to solve ordinary differential equations. These are of vital importance to a physicist as they are at the base of classical and celestial mechanics via Newton’s equations. They are also used to model electrical circuits and radioactive decay, amongst other systems.\nBroadly speaking, there are three ways to solve a differential equation: analytically, numerically (with a pre-existing function) or numerically (with a home-made function). Numerical approaches offer more flexibility but with the caveat that they are approximate and must be converged.\nIn this lesson we will use numerical approaches to solve ODE’s, with a strong focus on home-made numerical functions (rather than pre-made functions imported from a library). As such we will achieve a greater understanding of the underlying mathematics and approximations used - and it’s a great chance to practice our Python skills.\n\nBefore you begin\n\nCheck that Python and Jupyter Notebook are installed\nLaunch a Jupyter notebook\n\nPlease see the Setup page for more details.\n\n\nExternal resources\n📖 Textbook: Mark Newman’s Computational Physics sections 8.1, 8.2, 8.3, 8.4"
  },
  {
    "objectID": "markdown/ODE_questions.html",
    "href": "markdown/ODE_questions.html",
    "title": "ODE questions",
    "section": "",
    "text": "Use Euler’s method to solve the differential equation\n\n\\[\\begin{equation}\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} = -x^3 + \\mathrm{sin} t\n\\end{equation}\\]\nwith the initial condition \\(x=0\\) at \\(t=0\\).\n\nPlot \\(x(t)\\) up to \\(t=100\\), using 1000 steps."
  },
  {
    "objectID": "markdown/ODE_questions.html#eulers-method",
    "href": "markdown/ODE_questions.html#eulers-method",
    "title": "ODE questions",
    "section": "",
    "text": "Use Euler’s method to solve the differential equation\n\n\\[\\begin{equation}\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} = -x^3 + \\mathrm{sin} t\n\\end{equation}\\]\nwith the initial condition \\(x=0\\) at \\(t=0\\).\n\nPlot \\(x(t)\\) up to \\(t=100\\), using 1000 steps."
  },
  {
    "objectID": "markdown/ODE_questions.html#eulers-method-ii",
    "href": "markdown/ODE_questions.html#eulers-method-ii",
    "title": "ODE questions",
    "section": "Euler’s method II",
    "text": "Euler’s method II\n\nUse Euler’s method to solve the differential equation\n\n\\[\\begin{equation}\n\\frac{\\mathrm{d}y}{\\mathrm{d}x} = \\mathrm{cos}(x)\\mathrm{sin}(y) + \\mathrm{cos}(y)\n\\end{equation}\\]\nwith the initial condition \\(x=1\\) at \\(t=0\\).\n\nPlot \\(x(t)\\) up to \\(t=100\\), using 1000 steps."
  },
  {
    "objectID": "markdown/ODE_questions.html#second-order-runge-kutta-method",
    "href": "markdown/ODE_questions.html#second-order-runge-kutta-method",
    "title": "ODE questions",
    "section": "Second order Runge-Kutta method",
    "text": "Second order Runge-Kutta method\n\nUse a second order Runge-Kutta method to solve the differential equation\n\n\\[\\begin{equation}\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} = -x^3 + \\mathrm{sin} t\n\\end{equation}\\]\nwith the initial condition \\(x=0\\) at \\(t=0\\).\n\nPlot \\(x(t)\\) up to \\(t=100\\), using 1000 steps. Investigate convergence with respect to the number of steps, and compare this to the performance of Euler’s method."
  },
  {
    "objectID": "markdown/ODE_questions.html#modelling-a-non-linear-pendulum",
    "href": "markdown/ODE_questions.html#modelling-a-non-linear-pendulum",
    "title": "ODE questions",
    "section": "Modelling a non-linear pendulum",
    "text": "Modelling a non-linear pendulum\nIn the multivariable equations section we use Euler’s method to solve simultaneous first order ODEs. At the end of the tutorial you are also shown how to re-cast the second order ODE for a non-linear pendulum as two simultaneous first order ODEs. Using the given expressions on the tutorial page, and following the same method outlined as that outlined for the Strange Attractor, write a piece of code for modelling \\(\\theta\\) as a function of time.\nTip: Combine the two variable \\(\\theta\\) and \\(\\omega\\) into a single vector \\(\\mathbf{r} = (\\theta,\\omega)\\). The method will give us a value for \\(\\theta\\) and \\(\\omega\\), but we can ignore the \\(\\omega\\) values generated if they are not needed."
  },
  {
    "objectID": "markdown/ODE_questions.html#the-fourth-order-runge-kutta-method",
    "href": "markdown/ODE_questions.html#the-fourth-order-runge-kutta-method",
    "title": "ODE questions",
    "section": "The fourth order Runge-Kutta method",
    "text": "The fourth order Runge-Kutta method\nWe can extend the approach outlined in [the Runge-Kutta tutorial] to higher orders. The most common method for the numerical solution of ODEs is the fourth-order Runge-Kutta method (RK4). This is accurate to terms of order \\(h^4\\) (equivalently, it carries an error or order \\(h^5\\)). It’s derivation is quite tedious, but the approach is the same as that for the second-order method: we Taylor expand around various points and take linear combinations that lead to the cancellation of terms in \\(h^3\\), \\(h^4\\) and so on.\nThe five resulting equations are:\n\\[\\begin{equation}\nk_1 = hf(x,t)\n\\end{equation}\\]\n\\[\\begin{equation}\nk_2 = hf(x+\\frac{1}{2}k_1,t+\\frac{1}{2}h)\n\\end{equation}\\]\n\\[\\begin{equation}\nk_3 = hf(x+\\frac{1}{2}k_2,t+\\frac{1}{2}h)\n\\end{equation}\\]\n\\[\\begin{equation}\nk_4 = hf(x+k_3,t+h)\n\\end{equation}\\]\n\\[\\begin{equation}\nx(t+h) = x(t) + \\frac{1}{6}(k_1+2k_2+2k_3+k_4)\n\\end{equation}\\]\n\nUse RK4 to solve the differential equation\n\n\\[\\begin{equation}\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} = -x^3 + \\mathrm{sin} t\n\\end{equation}\\]\nwith the initial condition \\(x=0\\) at \\(t=0\\).\n\nPlot \\(x(t)\\) for a number of steps \\(N\\) equal to 10, 20, 50 and 1000.\nHow does convergence compare to that when using RK2? You may have already applied the RK2 method to this problem, as it was set as a question at the end of the tutorial."
  },
  {
    "objectID": "markdown/ODE_questions.html#dynamical-systems",
    "href": "markdown/ODE_questions.html#dynamical-systems",
    "title": "ODE questions",
    "section": "Dynamical systems",
    "text": "Dynamical systems\nThe motion of two bodies of mass \\(m_1\\) and \\(m_2\\) attracted by gravity is the Kepler problem. In this case the moving bodies experience a force varying as the inverse square of the distance between them; for body one the distance to body two is\n\\[\\begin{equation}\n\\mathbf{r}_{12} =  \\mathbf{r}_1-\\mathbf{r}_2,\n\\end{equation}\\]\nwhilst for body two the distance to body one is\n\\[\\begin{equation}\n\\mathbf{r}_{21} =  \\mathbf{r}_2-\\mathbf{r}_1.\n\\end{equation}\\]\nSo that the corresponding forces are:\n\\[\\begin{equation}\n\\mathbf{F}_1 = -\\frac{Gm_1m_2}{r_{12}}\\hat{\\mathbf{r_{12}}}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\mathbf{F}_2 = -\\frac{Gm_1m_2}{r_{21}}\\hat{\\mathbf{r_{21}}}\n\\end{equation}\\]\n\nNumerically compute and plot the orbits by integrating the equations of motion:\n\n\\[\\begin{equation}\nm_1\\mathbf{a}_1 = \\mathbf{F}_1,\n\\end{equation}\\]\n\\[\\begin{equation}\nm_2\\mathbf{a}_2 = \\mathbf{F}_2.\n\\end{equation}\\]\nUse the Euler method for the integration:\n\\[\\begin{equation}\n\\mathrm{v}_1(t+\\Delta t)= \\mathrm{v}_1(t)+\\Delta t \\frac{\\mathrm{F_1}}{m_1}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\mathrm{v}_2(t+\\Delta t)= \\mathrm{v}_2(t)+\\Delta t \\frac{\\mathrm{F_2}}{m_2}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\mathrm{r}_1(t+\\Delta t)= \\mathrm{r}_1(t)+\\Delta t \\mathrm{v}_1(t)\n\\end{equation}\\]\n\\[\\begin{equation}\n\\mathrm{r}_2(t+\\Delta t)= \\mathrm{r}_2(t)+\\Delta t \\mathrm{v}_2(t)\n\\end{equation}\\]\nAssume that the \\(z\\) axis is perpendicular to the plane of motion so that the motion is in two dimensions. Note that the force is not constant - it depends on \\(r\\), which couples together the equations for \\(\\mathbf{v}\\) and \\(\\mathbf{r}\\)\nFor the parameters assume \\(G=1\\), \\(m_1=1\\) and \\(m_2=1\\). For the initial conditions, assume that \\(r_1 = [1,0]\\), \\(r_2 = [-1,0]\\), \\(v_1 = [0,0.5]\\) and \\(v_2 = [0,-0.5]\\). Integrate up to the time \\(t_\\mathrm{max} = 20\\) with a time step size of \\(\\delta t = 0.05\\).\n\nOnce you have working code, change the initial velocities and comment on what you observe."
  },
  {
    "objectID": "markdown/recap_questions.html",
    "href": "markdown/recap_questions.html",
    "title": "Questions",
    "section": "",
    "text": "Altitude of a satellite\nA satellite is launched into a circular orbit around the earth so that it orbits the planet once every \\(T\\) seconds. The altitude \\(h\\) above the Earth’s surface that the satellite must have is:\n\\(h = \\left(\\frac{GMT^2}{4\\pi^2}\\right)^{\\frac{1}{3}} - R\\)\nwhere \\(G\\) is Newton’s gravitational constant, \\(M\\) is the mass of the Earth, and \\(R\\) is the radius of the Earth.\n\nWrite a piece of code which calculates the altitude \\(h\\) (in metres) for a given value of \\(T\\) (in seconds).\nUse this code to calculate the altitude of satellites that orbit the Earth once a day (a “geosynchronous” orbit) and once every 45 minutes. What can you conclude from this final calculation?\n\n\n\nThe emission lines of hydrogen\n\nThere is a simple and famous formula for calculating the wavelengths \\(\\lambda\\) of the emission lines of the hydrogen atom.\n\\(\\frac{1}{\\lambda} = R\\left(\\frac{1}{m^2} - \\frac{1}{n^2}\\right)\\)\nwhere R is the Rydberg constant \\(R = 1.097\\times 10^{-2}\\mathrm{nm}^{-1}\\) and \\(m\\) and \\(n\\) are positive integers and \\(n&gt;m\\).\n\nWrite a piece of code to calculate the first five transitions in the Lyman series (\\(m=1\\), transitions to the ground state, emission in the UV-range), Balmer series (\\(m=2\\), transitions to the first excited state, emissions in the visible region) and Paschen series (\\(m=3\\), emissions in the infra-red).\nCompare your answer to the energies indicated in the figure.\n\n\n\nCalculating Planck’s constant\nWhen light is shone on the surface of a metal, the photons in the light can excite (transfer energy to) electrons in the metal and, sometimes, eject them from the surface into the free space above. The energy of the ejected electron can be calculated by measuring the minimum voltage \\(V\\) that stops the electron moving.\nWe know that the energy of an ejected electron is equal to the energy of the photon that excited it minus the workfunction \\(\\phi\\) (which is the energy needed to remove it from the surface) and that the energy of a single photon is \\(hf\\) where \\(h\\) is Planck’s constant and \\(f\\) is the frequency of light. Mathematically this can be expressed as:\n\\(eV = hf - \\phi\\),\nwhere \\(e\\) is the charge of the electron.\n\nRead in the photoelectric measurement data from the file Planck.txt. The first column contains frequencies \\(f\\) is hertz and the second column contains voltages \\(V\\). Use this data to plot \\(V\\) vs \\(f\\). Think about the plot type - does a scatter plot of line plot make most sense?\n\nThe least-squares method is very commonly used for fitting a polynomial to a set of data. As it is so prevalent in physics and engineering, you are encouraged to watch this video which give an intuitive and mathematical description of the method.\n\nFit a straight line (polynomial of degree one) to the data using the least-squares method implemented in numpy.polyfit. Overlay this line on your data points.\nUsing the gradient of the fitted line calculate a value for Planck’s constant. Compare this to values you can find online.\n\n\n\nThe Madelung constant\nThe Madelung constant gives the total electric potential felt by an atom in a solid. It depends on the charge and position of other nearby atoms.\nConsider the compound sodium chloride. These are arranged on a cubic lattice, with sodium having a positive charge (\\(+e\\)) and chlorine having a negative charge (\\(-e\\)). If each atom position is given by integers \\((i,j,k)\\) then the sodium atoms are at positions where \\(i+j+k\\) is even and the chlorine atoms are at positions where \\(i+j+k\\) is odd.\n\nFor an atom at \\(i=j=k=0\\), the Madelung constant \\(M\\) can be approximated by using the following formulae:\n\\(V_\\mathrm{total} = \\sum_{i,j,k} V(i,j,k) = \\frac{e}{4\\pi\\epsilon_0a}M\\)\n\\(V(i,j,k) = \\pm\\frac{e}{4\\pi\\epsilon_0r}\\)\nwhere \\(r\\) is the distance from the origin to the atom at position \\((i,j,k)\\) and \\(a\\) is the atom spacing. The summation runs from \\(i,j,k=-L\\) to \\(i,j,k=L\\) but not including \\(i,j,k=0\\) (otherwise the expression would “blow up”).\n\nWrite an expression (in Markdown/LaTeX) for the distance \\(r\\) in terms of \\(i\\), \\(j\\), \\(k\\) and \\(a\\).\nCalculate the Madelung constant for sodium chloride using a large a value as L as you can (so the code runs in about a minute or less). How does it compare with published values?"
  },
  {
    "objectID": "markdown/PDE_introduction.html",
    "href": "markdown/PDE_introduction.html",
    "title": "Introduction to Partial Differential Equations",
    "section": "",
    "text": "It is a curious historical fact that modern quantum mechanics began with two quite different mathematical formulations: the differential equation of Schroedinger and the matrix algebra of Heisenberg. - Richard Feynman\n\nIn this section of the course we will learn how to solve another type of differential equation - the partial differential equation. These are also of vital importance to a physicist as they underly quantum mechanics (via the Schroedinger equation) and electromagnetism (via Maxwell’s equations). They are also used to model heat diffusion and wave propagation, amongst other processes.\nIt is usually impossible to write down explicit formulas for solutions of partial differential equations, and so there is a vast amount of research dedicated to solving these equations using computers. In this lesson we will start to explore some of these numerical approaches, with a focus on understanding the underlying mathematical methods used rather than importing pre-made functions.\n\nBefore you begin\n\nCheck that Python and Jupyter Notebook are installed\nLaunch a Jupyter notebook\n\nPlease see the Setup page for more details.\n\n\nExternal resources\n📖 Textbook: Mark Newman’s Computational Physics section 5.10"
  },
  {
    "objectID": "markdown/recap.html",
    "href": "markdown/recap.html",
    "title": "Python recap",
    "section": "",
    "text": "A pre-requisite for this course is a basic understanding of Python. If you need to recap on the basics, you can use the Python for Physicists website - this covers all of the pre-requisite knowledge needed.\nTo self-assess your level of understanding, we recommend using ChooChoo the Checklist tool."
  },
  {
    "objectID": "markdown/differentiation_questions.html",
    "href": "markdown/differentiation_questions.html",
    "title": "Differentiation questions",
    "section": "",
    "text": "Write a program that defines a function \\(f(x)\\) returning the value \\(x(x-1)\\), then uses the backwards difference method to calculate the numerical derivative of \\(f(x)\\) at the point \\(x=1\\), with \\(h=10^{-2}\\).\nCalculate the exact value of the derivative and compare this to the answer your program gives. The two will not agree perfectly - why not?\nRepeat the calculation for \\(h=10^{-2},10^{-4},10^{-6},10^{-8},10^{-10},10^{-12},10^{-14}\\). What do you observe about the accuracy of the calculation? Why does it behave in this way?"
  },
  {
    "objectID": "markdown/differentiation_questions.html#exploring-errors",
    "href": "markdown/differentiation_questions.html#exploring-errors",
    "title": "Differentiation questions",
    "section": "",
    "text": "Write a program that defines a function \\(f(x)\\) returning the value \\(x(x-1)\\), then uses the backwards difference method to calculate the numerical derivative of \\(f(x)\\) at the point \\(x=1\\), with \\(h=10^{-2}\\).\nCalculate the exact value of the derivative and compare this to the answer your program gives. The two will not agree perfectly - why not?\nRepeat the calculation for \\(h=10^{-2},10^{-4},10^{-6},10^{-8},10^{-10},10^{-12},10^{-14}\\). What do you observe about the accuracy of the calculation? Why does it behave in this way?"
  },
  {
    "objectID": "markdown/differentiation_questions.html#the-method-of-finite-differences-for-second-order-derivatives",
    "href": "markdown/differentiation_questions.html#the-method-of-finite-differences-for-second-order-derivatives",
    "title": "Differentiation questions",
    "section": "The method of finite differences for second order derivatives",
    "text": "The method of finite differences for second order derivatives\nIn the tutorial we give an expression for calculating second order derivatives using the finite difference method:\n\\[\\begin{equation}\n\\frac{\\mathrm{d} ^2f}{\\mathrm{d} x^2} \\simeq \\frac{f(x+h)-2f(x)+f(x-h)}{h^2}.\n\\end{equation}\\]\nUsing the fact that a second derivative is, by definition, a derivative of a derivative, and by applying the central difference method multiple times, derive this expression."
  },
  {
    "objectID": "markdown/differentiation_introduction.html",
    "href": "markdown/differentiation_introduction.html",
    "title": "Introduction to differentiation",
    "section": "",
    "text": "The origin of the notion of derivatives is in the vague feeling of the mobility of things, and of the greater or less speed with which phenomena take place. - Émile Picard\n\nIn physics we are often looking at how things change over time or space so, like integrals, the evaluation of derivatives is one of the most important applications of computers in physics - especially when solving partial differential equations (a topic we will cover later in the course). However, unlike integrals, the derivatives of known functions can always be calculated analytically, so there’s generally less need to calculate them numerically.\nIn this lesson we will learn how to calculate derivatives using finite difference methods, and take a brief look at the related operation of interpolation. There are some significant practical problems with numerical derivatives; at the end of this lesson we will also learn how to evaluate the accuracy and speed that can be achieved when using these various methods.\n\nBefore you begin\n\nCheck that Python and Jupyter Notebook are installed\nLaunch a Jupyter notebook\n\nPlease see the Setup page for more details.\n\n\nExternal resources\n📖 Textbook: Mark Newman’s Computational Physics section 5.10"
  },
  {
    "objectID": "markdown/integration_introduction.html",
    "href": "markdown/integration_introduction.html",
    "title": "Introduction to integration",
    "section": "",
    "text": "If one looks at the different problems of the integral calculus which arise naturally when one wishes to go deep into the different parts of physics, it is impossible not to be struck by the analogies existing. - Henri Poincaré\n\nOne of the most basic but also most important applications of computers in physics is the evaluation of integrals. Integrals occur widely in physics and, while some integrals can be done analytically, many cannot. In this lesson we will learn several methods for numerical integration and, as part of this, we will learn how to generate random numbers.\n\nBefore you begin\n\nCheck that Python and Jupyter Notebook are installed\nLaunch a Jupyter notebook\n\nPlease see the Setup page for more details.\n\n\nExternal resources\n📖 Textbook: Mark Newman’s Computational Physics sections 5.1-5.4 and 10.2"
  },
  {
    "objectID": "markdown/integration_questions.html",
    "href": "markdown/integration_questions.html",
    "title": "Integration questions",
    "section": "",
    "text": "Planck’s law tells us that in the angular frequency interval \\(\\omega\\) to \\(\\omega+\\mathrm{d}\\omega\\), a black-body of unit area and temperature \\(T\\) radiates electromagnetically an amount of thermal energy per second equal to \\(I(\\omega)\\mathrm{d}\\omega\\), where\n\\[I(\\omega) = \\frac{\\hbar}{4\\pi^2\\mathrm{c}^2}\\frac{\\omega}{(\\mathrm{e}^{\\frac{\\hbar\\omega}{k_\\mathrm{B}T}}-1)}.\\]\nThe quantum theory of modern physics was born with Planck’s law. Up until this point, an outstanding problem in physics was the ultra-violet catastrophe, a prediction from classical physics that a black body at thermal equilibrium would emit an unbounded quantity of energy as wavelength decreased into the ultraviolet range. This was in stark contrast to what was being measured experimentally. To resolve this, Planck assumed that electromagnetic radiation can only be absorbed or emitted in discrete packets (quanta) and from this derives Planck’s law as shown above. Planck’s law was in agreement with experimental results and the Stefan-Boltzmann law, which states:\n\\[W = \\sigma T^4\\]\nwhere \\(\\sigma\\) is the Stefan-Boltzmann constant.\nEinstein later postulated that the discrete quanta are real, physical particles (photons) and used this to explain the photoelectric effect. This research resulted in Einstein receiving the Nobel prize for physics in 1921.\nBy substituting \\(x = \\frac{\\hbar\\omega}{k_\\mathrm{B}T}\\) we can deduce that the total rate of energy radiation by a black body per unit area, over all frequencies, is\n\\[W = \\frac{k_\\mathrm{B}^4T^4}{4\\pi^2\\mathrm{c}^2\\hbar^3}\\int_0^\\infty\\frac{x^3}{(\\mathrm{e}^x-1)}\\mathrm{d}x.\\]\n\nWe will approximate the exact expression, an integral from 0 to \\(\\infty\\), using an integral with finite limits. Plot the integrand as a function of \\(x\\) to justify your choice of limits.\nUse the rectangular slice method to evaluate the integral in the expression for \\(W\\)\nUse your value for the integral above to compute a value for the Stefan-Boltzmann constant to three significant figures. Check your result against the known value (available in the scipy.constants library).\nShow that the error of your estimate scales linearly with the width \\(h\\) of the rectangular slices used to approximate the integral."
  },
  {
    "objectID": "markdown/integration_questions.html#the-stefan-boltzmann-constant",
    "href": "markdown/integration_questions.html#the-stefan-boltzmann-constant",
    "title": "Integration questions",
    "section": "",
    "text": "Planck’s law tells us that in the angular frequency interval \\(\\omega\\) to \\(\\omega+\\mathrm{d}\\omega\\), a black-body of unit area and temperature \\(T\\) radiates electromagnetically an amount of thermal energy per second equal to \\(I(\\omega)\\mathrm{d}\\omega\\), where\n\\[I(\\omega) = \\frac{\\hbar}{4\\pi^2\\mathrm{c}^2}\\frac{\\omega}{(\\mathrm{e}^{\\frac{\\hbar\\omega}{k_\\mathrm{B}T}}-1)}.\\]\nThe quantum theory of modern physics was born with Planck’s law. Up until this point, an outstanding problem in physics was the ultra-violet catastrophe, a prediction from classical physics that a black body at thermal equilibrium would emit an unbounded quantity of energy as wavelength decreased into the ultraviolet range. This was in stark contrast to what was being measured experimentally. To resolve this, Planck assumed that electromagnetic radiation can only be absorbed or emitted in discrete packets (quanta) and from this derives Planck’s law as shown above. Planck’s law was in agreement with experimental results and the Stefan-Boltzmann law, which states:\n\\[W = \\sigma T^4\\]\nwhere \\(\\sigma\\) is the Stefan-Boltzmann constant.\nEinstein later postulated that the discrete quanta are real, physical particles (photons) and used this to explain the photoelectric effect. This research resulted in Einstein receiving the Nobel prize for physics in 1921.\nBy substituting \\(x = \\frac{\\hbar\\omega}{k_\\mathrm{B}T}\\) we can deduce that the total rate of energy radiation by a black body per unit area, over all frequencies, is\n\\[W = \\frac{k_\\mathrm{B}^4T^4}{4\\pi^2\\mathrm{c}^2\\hbar^3}\\int_0^\\infty\\frac{x^3}{(\\mathrm{e}^x-1)}\\mathrm{d}x.\\]\n\nWe will approximate the exact expression, an integral from 0 to \\(\\infty\\), using an integral with finite limits. Plot the integrand as a function of \\(x\\) to justify your choice of limits.\nUse the rectangular slice method to evaluate the integral in the expression for \\(W\\)\nUse your value for the integral above to compute a value for the Stefan-Boltzmann constant to three significant figures. Check your result against the known value (available in the scipy.constants library).\nShow that the error of your estimate scales linearly with the width \\(h\\) of the rectangular slices used to approximate the integral."
  },
  {
    "objectID": "markdown/integration_questions.html#volume-of-a-sphere",
    "href": "markdown/integration_questions.html#volume-of-a-sphere",
    "title": "Integration questions",
    "section": "Volume of a sphere",
    "text": "Volume of a sphere\nWe have seen that performing an integral over one variable requires us to take samples across a one-dimensional grid. Similarly, integration over two variables requires us to take samples on a two-dimensional grid, and so on. The number of grid points required scales exponentially with the number of variables, so that integrals over three or more dimensions can quickly become unmanageable. In this case, Monte Carlo integration can be very useful.\nThe volume of a sphere with unit radius in three dimensions is given by:\n\\[V = \\int\\int\\int_{-1}^1f(x,y,z)\\mathrm{d}x\\mathrm{d}y\\mathrm{d}z\\]\nwhere \\(f(x,y,z)=1\\) everywhere inside the sphere and zero everywhere outside.\n\nUse Monte Carlo integration to estimate the volume of this sphere using 100 random points.\nIncrease the number of random points until you reach an estimate that is exact to three significant figures.\nBy considering how many points would need to be sampled, estimate the length of time it would take to reach the same level of accuracy using a Riemann slice type method."
  },
  {
    "objectID": "markdown/integration_questions.html#the-stefan-boltzmann-constant-re-visited",
    "href": "markdown/integration_questions.html#the-stefan-boltzmann-constant-re-visited",
    "title": "Integration questions",
    "section": "The Stefan-Boltzmann constant re-visited",
    "text": "The Stefan-Boltzmann constant re-visited\nIn the Riemann sums tutorial question we use rectangular slices to estimate the Stefan-Boltzmann constant to three significant figures. The trapezoidal rule is a neat extension to rectangular slices, where the value at the start and end of each segment is considered. It can be expressed as\n\\[\\int^b_af(x)\\mathrm{d}x = h\\left[\\frac{1}{2}f(a)+\\frac{1}{2}f(b)+\\sum_{k=1}^{N-1}f(a+kh)\\right]\\]\n\nUse the trapezoidal rule to re-calculate the Stefan-Boltzmann constant to three significant figures\nDemonstrate that the error scales quadratically with \\(h^2\\), and that the trapezoidal rule is more computationally efficient than using rectangular slices\nThe error \\(\\epsilon_n\\) on the nth estimate of an integral is given by \\(\\epsilon_n = \\frac{1}{3}(I_n-I_{n-1})\\). Using this expression, write a programme that calculates the Stefan-Boltzmann with a maximum absolute error of 1E-5."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Physics",
    "section": "",
    "text": "Figure: A simulation of vinegar-oil separation after mixing. This uses the Cahn-Hilliard partial differential equation to model the diffusive dynamics of a two-phase system.\n\n\nComputing has become central to virtually all research and development in academia and industry, and with the advent of Machine Learning and High-Performance Computing this dominance is set to continue.\nComputational physics encompasses a wide number of areas including materials modelling, particle physics simulations, protein structure prediction and plasma modelling. In fact, it is possible to find a computational branch for every major field in physics.\nThe focus of this course is to equip with you with knowledge and skills needed to model a range of physical systems using numerical methods. You will also gain a number of transferable skills which can be applied across in a range of computational disciplines.\nThis course is split into three sections - Getting started, Getting results and Getting it out there. In the first part of the course, Getting started, we will recap the basic programming concepts and Python libraries that are applicable to a wide range of research and industry. In the middle section of the course, which is called Getting results, we will combine our Python knowledge with mathematical numerical methods to model various physical systems using differential equations. In the final unassessed section of the course, Getting it out there, we will use modern software engineering techniques to document, test and share our code.\n\nHow does this course relate to experimental physics?\nThe first part of this course recaps the basic skills needed for processing experimental data using Python. At the end of Getting started you will be able to read in experimental data from a plain text file, clean the data, apply basic statistical analysis and plot. We very much encourage you to apply the tools outlined in this course to your experimental analysis.\n\n\nDo I need to attend the in-person labs?\nWe will aim to publish all course materials on this website however a large component of this course is based on the verbal explanations whilst writing code (a.k.a, “live coding”) and class discussion. It will all make much more sense if you attend the sessions in the computer lab. The computer labs will not be recorded.\n\n\nWhy do you use a website?\nBlackboard collaborate is widely used at Northumbria University for teaching. However in the computational sciences (and in software engineering) Blackboard collaborate is not widely used. The aim here is to teach and learn using the tools that are already used in research and industry - hence building the course resources around the Github service. Also, the code can be nicely formatted using Markdown/html (rather than blackboard), making it easier for students to read and understand.\nThe source code for this website can be found in the course Github repository. If you spot any mistakes or would like to make a suggestions for improving the course please raise an issue on Github.\nBlackboard collaborate will be used for posting assessments and for course announcements."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "To participate in this course you will need access to Python and an up-to-date web browser. You will also need a range of Python libraries from the standard scientific stack: Jupyter, Numpy, Scipy and Matplotlib.\nAll of the software needed for this course is pre-installed in the MPEE computers at Northumbria University. However we highly recommend you install Python on your own laptop or computer so that you can work from home, or from a cafe - wherever you work best! To install Python and the scientific libraries on your personal laptop or desktop carefully follow the instructions listed below.\nYou can also run the Python Jupyter Notebook files remotely through the binder or colab services whenever you see the icon for these services. But be warned! Any changes you make are not saved and the service can time-out after a period of inactivity (usually ~20 minutes). This service should be used as a if-nothing-else-works plan only.\nYou will also need a user account at github.com - Basic GitHub accounts are free.\n\nPython\nPython is a popular language for scientific computing, and great for general-purpose programming as well. Installing all of its scientific packages individually can be a bit difficult, however, so we recommend the all-in-one installer Anaconda.\n\nWindows - video tutorial\n\nOpen this link with your web browser.\nDownload the Anaconda for Windows installer with Python 3. (If you are not sure which version to choose, you probably want the 64-bit Graphical Installer Anaconda3-…-Windows-x86_64.exe)\nDouble-click the executable and install Python 3 using MOST of the default settings, the only exception is to check Add Anaconda to my PATH environment variable.\n\n\n\nMac OS X - video tutorial\n\nOpen this link with your web browser.\nDownload the Anaconda Installer with Python 3 for macOS (you can either use the Graphical or the Command Line Installer).\nInstall Python 3 by running the Anaconda Installer using all of the defaults for installation.\n\n\n\n\nA browser for Jupyter\nWe will teach Python using the [Jupyter notebook][https://jupyter.org/], a programming environment that runs in a web browser. Jupyter requires a reasonably up-to-date browser, preferably a current version of Chrome, Safari, or Firefox (note that Internet Explorer version 9 and below are not supported). Jupyter is installed as part of the Anaconda package for Python.\n\n\nHow to launch a Jupyter Notebook\n\nSearch for “Anaconda” using your system search bar\nSelect “Anaconda Launcher”\nClick on the “Launch” button below “Jupyter Notebook”. You should see a file browser pop up as a new tab on your browser.\nClick new towards the top right hand side of the browser window.\nSelect Python 3 in the drop-down menu bar. This will open a Python 3 Notebook file a new tab in your browser."
  },
  {
    "objectID": "notebooks/euler_method.html",
    "href": "notebooks/euler_method.html",
    "title": "Euler’s method",
    "section": "",
    "text": "Questions\n\n\n\n\nHow do I use Euler’s method to solve a first-order ODE?\nWhat does the term first-order accurate mean?\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\nUse Euler’s method, implemented in Python, to solve a first-order ODE\nUnderstand that this method is approximate and the significance of step size \\(h\\)\nCompare results at different levels of approximation using the matplotlib library.\n\n\n\n\nThere are a variety of ways to solve an ODE\nIn the previous lesson we considered nuclear decay:\n\\[\\begin{equation}\n\\frac{\\mathrm{d} N}{\\mathrm{d} t} = -\\lambda N\n\\end{equation}\\]\nThis is one of the simplest examples of am ODE - a first-order, linear, separable differential equation with one dependent variable. We saw that we could model the number of atoms \\(N\\) by finding an analytic solution through integration:\n\\[\\begin{equation}\nN = N_0 e^{-\\lambda t}\n\\end{equation}\\]\nHowever there is more than one way to crack an egg (or solve a differential equation). We could have, instead, used an approximate, numerical method. One such method - Euler’s method - is this subject of this lesson.\n\n\nA function can be approximated using a Taylor expansion\nThe Taylor series is a polynomial expansion of a function about a point. For example, the image below shows \\(\\mathrm{sin}(x)\\) and its Taylor approximation by polynomials of degree 1, 3, 5, 7, 9, 11, and 13 at \\(x = 0\\).\n\n\n\nCredit: Image By IkamusumeFan - CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=27865201\n\n\nThe Taylor series of \\(f(x)\\) evaluated at point \\(a\\) can be expressed as:\n\\[\\begin{equation}\nf(x) = f(a) + \\frac{\\mathrm{d} f}{\\mathrm{d} x}(x-a) + \\frac{1}{2!} \\frac{\\mathrm{d} ^2f}{\\mathrm{d} x^2}(x-a)^2 + \\frac{1}{3!} \\frac{\\mathrm{d} ^3f}{\\mathrm{d} x^3}(x-a)^3\n\\end{equation}\\]\nReturning to our example of nuclear decay, we can use a Taylor expansion to write the value of \\(N\\) a short interval \\(h\\) later:\n\\[\\begin{equation}\nN(t+h) = N(t) + h\\frac{\\mathrm{d}N}{\\mathrm{d}t} + \\frac{1}{2}h^2\\frac{\\mathrm{d}^2N}{\\mathrm{d}t^2} + \\ldots\n\\end{equation}\\]\n\\[\\begin{equation}\nN(t+h) = N(t) + hf(N,t) + \\mathcal{O}(h^2)\n\\end{equation}\\]\n\n\n\n\n\n\nNote\n\n\n\nIf you want to know more about Taylor expansion, there is an excellent video explanation from user 3blue1brown on Youtube, a link is provided under external resources.\n\n\n\n\nIf the step size \\(h\\) is small then higher order terms can be neglected\nIf \\(h\\) is small and \\(h^2\\) is very small we can neglect the terms in \\(h^2\\) and higher and we get:\n\\[\\begin{equation}\nN(t+h) = N(t) + hf(N,t).\n\\end{equation}\\]\n\n\nEuler’s method can be used to approximate the solution of differential equations\nWe can keep applying the equation above so that we calculate \\(N(t)\\) at a succession of equally spaced points for as long as we want. If \\(h\\) is small enough we can get a good approximation to the solution of the equation. This method for solving differential equations is called Euler’s method, after Leonhard Euler, its inventor.\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough we are neglecting terms \\(h^2\\) and higher, Euler’s method typically has an error linear in \\(h\\) as the error accumulates over repeated steps. This means that if we want to double the accuracy of our calculation we need to double the number of steps, and double the calcuation time.\n\n\n\n\n\n\n\n\nNote\n\n\n\nSo far we have looked at an example where the input (or independent variable) is time. This isn’t always the case - but it is the most common case in physics, as we are often interested in how things evolve with time.\n\n\n\n\nEuler’s method can be applied using the Python skills we have developed\nLet’s use Euler’s method to solve the differential equation for nuclear decay. We will model the decay process over a period of 10 seconds, with the decay constant \\(\\lambda=0.1\\) and the initial condition \\(N_0 = 1000\\).\n\\[\\begin{equation}\n\\frac{\\mathrm{d}N}{\\mathrm{d} t} = -0.1 N\n\\end{equation}\\]\nFirst, let’s import the standard scientific libraries we will be using - Numpy and Matplotlib:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nLet’s definte the function \\(f(N,t)\\) which describes the rate of decay. In this case, the function depends only on the number of atoms present.\n\n# define the function for nuclear decay\ndef f(Num_atoms):\n    return -0.1*Num_atoms\n\nNext we’ll list the simulation parameters and initial conditions: start time, end time, number of starting atoms (which is an initial condition), number of time steps and step size (which is calculated using the number of time steps).\n\na = 0                  # start time\nb = 10                 # end time\nNum_atoms = 1000       # initial condition\nnum_steps = 5         # number of time steps\nh = (b-a) / num_steps  # time step size\n\nWe use the Numpy arange function to generate a list of evenly spaced times at which to evaluate the number of atoms. We also create an empty list to hold the values for \\(N\\) that we are yet to calculate.\n\n# use the Numpy arange function to generate a list of evenly spaced times at which to evaluate the number of atoms N.\ntime_list = np.arange(a,b,h)\n\n# create an empty list to hold the calculated N values\nNum_atoms_list = []\n\nFinally, we apply Euler’s method using a For loop. Note that the order of operations in the loop body is important.\n\n# apply Euler's method. Note that the order of operations in the loop body is important.\nfor time in time_list:\n    Num_atoms_list.append(Num_atoms)\n    Num_atoms += h*f(Num_atoms)\n\n\n\nWe can easily visualise our results, and compare against the analytical solution, using the matplotlib plotting library\n\nplt.scatter(time_list, Num_atoms_list)\nplt.xlabel(\"time\")\nplt.ylabel(\"Number of atoms\")\nplt.show()\n\n\n\n\nUsing the analytic solution from the previous lesson, we can define a function for calculating the number of atoms \\(N\\) as a function of time (this is the exact solution).\n\ndef analytic_solution(time):\n    return 1000*np.exp(-0.1*time)\n\nWe can use this to calculate the exact value for \\(N\\) over the full time range. Note that we use a large number of points in time (in this case 1000) to give a nice smooth curve:\n\nnum_steps = 1000\nh = (b-a) / num_steps\ntime_analytic_list = np.arange(a,b,h)\nNum_atoms_analytic_list = []\n\nfor time in time_analytic_list:\n    Num_atoms_analytic_list.append(analytic_solution(time))\n\nFinally, we plot the approximate Euler method results against the exact analytical solution:\n\nplt.plot(time_analytic_list,Num_atoms_analytic_list)\nplt.scatter(time_list, Num_atoms_list)\nplt.xlabel(\"time\")\nplt.ylabel(\"Number of atoms\")\n\nText(0, 0.5, 'Number of atoms')\n\n\n\n\n\nWe can see that the error is increasing over time. We can calculate the error at \\(t=8\\):\n\nprint(\"Analytic solution at t=8: \",round(analytic_solution(8)))\nprint(\"Numerical solution at t=8: \",round(Num_atoms_list[-1]))\nprint(\"Error is: \",round(analytic_solution(8)-Num_atoms_list[-1]))\n\nAnalytic solution at t=8:  449\nNumerical solution at t=8:  410\nError is:  40\n\n\n\n\nEuler’s method is a first-order method accurate to order \\(h\\).\n\nNumerical methods give approximate solutions.\nEuler’s method neglects the term in \\(h^2\\) and higher: \\[\\begin{equation}\nx(t+h) = x(t)+hf(x,t)+\\mathcal{O}(h^2)\n\\end{equation}\\]\nThis tells us the error introduced on a single step of the method is proportional to \\(h^2\\) - this makes Euler’s method a  first-order  method, accurate to order \\(h\\).\nHowever the cumulative error over several steps is proportional to \\(h\\)\nSo to make our error half as large we need to double the number of steps (halve the step size) and double the length of the calculation.\n\n\n\n\n\n\n\nKeypoints\n\n\n\n\nThere are a variety of ways to solve an ODE\nA function can be approximated using a Taylor expansion\nIf the step size \\(h\\) is small then higher order terms can be neglected\nEuler’s method can be used to approximate the solution of differential equations\nEuler’s method can be applied using the Python skills we have developed\nWe can easily visualise our results, and compare against the analytical solution, using the matplotlib plotting library\nEuler’s method is a first-order method accurate to order \\(h\\)."
  },
  {
    "objectID": "notebooks/multivariable_equations.html",
    "href": "notebooks/multivariable_equations.html",
    "title": "Multivariable equations",
    "section": "",
    "text": "Questions\n\n\n\n\nHow do I solve simultaneous ODEs?\nHow do I solve second-order ODEs (and higher)?\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\nUse Euler’s method, implemented in Python, to solve a set of simultaneous ODEs\nUse Euler’s method, implemented in Python, to solve a second-order ODE\nUnderstand how the same method could be applied to higher order ODEs\n\n\n\n\nComputers don’t care so much about the type of differential equation\nIn the previous lesson we used Euler’s method to model radioactive decay. To demonstrate the method we deliberately chose a simple example (a linear, first order, separable ODE with one dependent variable). We have seen that this equation can also be solved analytically, so really there is no need for approximate numerical methods in this case.\nHowever there are a large number of physical equations that cannot be solved analytically, and that rely on numerical methods for their modelling.\nFor example, the Lotka-Volterra equations for studying predator-prey interactions have multiple dependent variables and the Cahn-Hilliard equation for modelling phase separation in fluids is non-linear. These equations can be solved analytically for particular, special cases only.\nComputers don’t care whether an equation is linear or non-linear, multi-variable or single-variable, the numerical method for studying it is much the same - with the caveat that numerical methods are approximate and so we need to think about the accuracy of the methods used.\n\n\nSimultaneous ODEs can be solved using numerical methods\nOne class of problems that are difficult to solve analytically are simultaenous ODEs. These are equations where the derivative of each dependent variable can depend on any of the variables (dependent or independent). For example,\n\\[\\begin{eqnarray}\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} &=& xt + y \\\\\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} &=& \\mathrm{sin}^2\\omega t - xy\n\\end{eqnarray}\\]\nA famous set of simulataneous ODEs are the Lorenz equations, which are the subject of this lesson.\n\\[\\begin{eqnarray}\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} &=& \\sigma(y-x) \\\\\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} &=& rx-y-xz \\\\\n\\frac{\\mathrm{d}z}{\\mathrm{d}t} &=& xy-bz\n\\end{eqnarray}\\]\nThere three dependent variables - \\(x\\), \\(y\\) and \\(z\\), and one independent variable \\(t\\). There are also three constants - \\(\\sigma\\), \\(r\\) and \\(b\\).\nFor particular values of \\(\\sigma\\), \\(r\\) and \\(b\\) the Lorenz systems has chaotic behaviour (a strong dependence on the initial conditions) and for a subset of these, there is also fractal structure called the strange attractor.\n\n\nEuler’s method is easily extended to simultaneous ODEs\nIn the previous lesson we were introduced to Euler’s method for the single variable case:\n\\[\\begin{equation}\nN(t+h) = N(t) + hf(N,t).\n\\end{equation}\\]\nThis can be easily extended to the multi-variable case using vector notation:\n\\[\\begin{equation}\n\\mathbf{r}(t+h) = \\mathbf{r}(t) + h\\mathbf{f}(\\mathbf{r},t).\n\\end{equation}\\]\nWe have seen that arrays can be easily represented in Python using the NumPy library. This allows us to do arithmetic with vectors directly (rather than using verbose workarounds such as for loops), so the code is not much more complicated than the one-variable case.\nFirst, let’s import the standard scientific libraries we will be using - Numpy and Matplotlib:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nNow we define the function that describe the Lorenz system.\n\n\n\n\n\n\nNote\n\n\n\nWhen the operands are NumPy arrays, the operations are automatically element-wise vector operations.\n\n\n\n# define the Lorenz system\n\ndef Lorenz(sigma,r,b,xyz):\n    \n    x = xyz[0]\n    y = xyz[1]\n    z = xyz[2]\n    \n    fx = sigma*(y-x)\n    fy = (r*x)-y-(x*z)\n    fz = (x*y)-(b*z)\n    \n    return np.array([fx,fy,fz],float)\n\nNext we’ll list the simulation parameters (start time, end time, number of time steps and step size), the initial values for \\(x\\), \\(y\\) and \\(z\\), and the constants \\(\\sigma\\), \\(r\\) and \\(b\\).\n\n# calculation parameters\nstart = 0                  # start time\nend = 50                 # end time\nnum_steps = 3000         # number of time steps\nh = (end-start) / num_steps  # time step size\n\n# intitial conditions: x=0, y=1, z=0\nxyz = np.array([0,1,0],float)\n\n# constants\nsigma = 10\nr = 28\nb = 8/3\n\nWe use the Numpy arange function to generate a list of evenly spaced times at which to evaluate the system. We also create an empty list to hold the values for \\(x\\), \\(y\\) and \\(z\\) that we are yet to calculate.\n\n# use the Numpy arange function to generate a list of evenly spaced times at which to evaluate the number of atoms N.\ntime_list = np.arange(start,end,h)\n\n# create empty arrays to hold the calculated values\nx_points = []\ny_points = []\nz_points = []\n\nFinally, we apply Euler’s method using a For loop. Note that the order of operations in the loop body is important.\n\nfor time in time_list:\n    \n    x_points.append(xyz[0])\n    y_points.append(xyz[1])\n    z_points.append(xyz[2])\n    xyz += h*Lorenz(sigma,r,b,xyz)\n\nTo visualise the strange attractor we can plot the values of \\(z\\) against \\(y\\):\n\nplt.plot(x_points,z_points)\n\n\n\n\n\n\nHigher order ODEs can be re-cast as simultaneous ODEs and solved in the same way\nMany physical equations are second-order or higher. The general form for a second-order differential equation with one dependent variable is:\n\\[\\begin{equation}\n\\frac{\\mathrm{d}^2x}{\\mathrm{d}t^2} = f\\left(x,\\frac{\\mathrm{d}x}{\\mathrm{d}t},t\\right)\n\\end{equation}\\]\nLuckily, we can re-cast a higher order equation as a set of simultaneous equations, and then solve in the same way as above.\nLet’s use the non-linear pendulum as an example. For a pendulum with an art of length \\(l\\) and a bob of mass \\(m\\), Newton’s second law (\\(F=ma\\)) provides the following equation of motion:\n\\[\\begin{equation}\nml\\frac{\\mathrm{d}^2\\theta}{\\mathrm{d}t^2} = -mg\\sin(\\theta),\n\\end{equation}\\]\nwhere \\(\\theta\\) is the angle of displacement of the arm from the vertical and \\(l\\frac{\\mathrm{d}^2\\theta}{\\mathrm{d}t^2}\\) is the acceleration of the mass in the tangential direction. The exact solution to this equation is unknown, but we now have the knowledge needed to find a numerical approximation.\nThe equation can be re-written as:\n\\[\\begin{equation}\n\\frac{\\mathrm{d}^2\\theta}{\\mathrm{d}t^2} = -\\frac{g}{l}\\sin(\\theta)\n\\end{equation}\\]\nWe define a new variable \\(\\omega\\):\n\\[\\begin{equation}\n\\frac{\\mathrm{d}\\theta}{\\mathrm{d}t} = \\omega\n\\end{equation}\\]\nand substitute this into the equation of motion:\n\\[\\begin{equation}\n\\frac{\\mathrm{d}\\omega}{\\mathrm{d}t} = -\\frac{g}{l}\\sin(\\theta).\n\\end{equation}\\]\nThese two expressions form a set of simultaneous equations that can be solved numerically using the method outlined above.\n\n\n\n\n\n\nKeypoints\n\n\n\n\nComputers don’t care so much about the type of differential equation\nSimulatenous ODEs can also be solved using numerical methods\nEuler’s method is easily extended to the multi-variable case\nHigher order ODEs can be re-cast as simultaneous ODEs and solved the same way"
  },
  {
    "objectID": "notebooks/monte_carlo.html",
    "href": "notebooks/monte_carlo.html",
    "title": "Monte Carlo",
    "section": "",
    "text": "Questions\n\n\n\n\nHow do I generate random numbers?\nHow do I integrate using Monte Carlo methods?\nWhen might Monte Carlo integration be useful?\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\nUse the random module to generate random numbers\nUse Monte Carlo methods to calculate the area of a circle\n\n\n\n\nThere are many different numerical methods for calculating integrals\nIn the previous section we studied the simplest methods for calculating integrals: the rectangular-slice method . For increased accuracy and computational efficiency, there are extensions to this approach - for example, the trapezoid method (where each slice is a trapezoid rather than rectangle) or Simpson’s rule (where a quadratic curve is fitted to each slice). For certain classes of functions we can increase the performance further using more specialised approaches such as Gaussian Quadrature.\nOne particularly flexible and general purpose approach for calculating integrals is to use Monte Carlo integration. This approach is useful when the integrand is “pathological” (wildly varying) or noisy, or when the integration is performed over several variables.\n\n\nMonte Carlo methods calculate the answers to exact calculations by doing random calculations\nMonte Carlo methods are a broad class of algorithms that rely on random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle. “Monte Carlo” is a reference to a well-known casino town, since the element of chance is core to the modelling approach, similar to various casino games.\nMonte Carlo methods are applied across a wide variety of domains, most commonly mathematics, physics and finance. In physics, Monte Carlo methods are used to design particle detectors, model galaxy evolution and solve the many-body problem for quantum systems, amongst many other applications. In this lesson we will introduce one of the main uses of Monte Carlo: for integration.\n\n\nThe Monte Carlo “area method” estimates integrals by generating a uniform sample of points and counting how many fall into a planar region\nConsider the shaded area as shown below. This is the integral \\(I\\) which we wish to calculate.\n\nIf we choose a point uniformly at random in the rectange (dashed red line) that bounds the shaded area, the probability \\(p\\) that the point falls in the shaded area is\n\\[p = \\frac{I}{A}\\]\nwhere \\(A\\) is the area of the bounding rectangle. This means that the integral \\(I\\) can be calculated if we know the area of the bounding rectangle and the probability \\(p\\):\n\\[I = Ap\\]\nTo calculate \\(p\\) we can randomly generate \\(N\\) points in the bounding area and keep count as to how many lie in the shaded area. If \\(k\\) lie in the shaded area then the fraction of points \\(\\frac{k}{N}\\) should be equal to the probability \\(p\\).\n\\[I = \\frac{Ak}{N}\\]\nWe can extend this approach to higher dimensions to consider integrands lying within a bounding volume, or higher dimensional space (which is considered in the extension exercise for this lesson).\n\n\nWe can use the Monte Carlo area method to estimate pi\nWe’ll now use this approach to give us an estimate for the value of pi by considering the area under a quarter-circle.\n\n\n\nThe relevant equations are:\nsquare area: \\(A_s = (2 r)^2\\)\ncircle area: \\(A_c = \\pi r^2\\)\nThe ratio of the areas can be related to \\(\\pi\\) through the following expressions:\n\\[\\frac{A_c}{A_s} = \\frac{\\pi r^2}{4 r^2} = \\frac{\\pi}{4}\\]\n\\[\\pi = 4\\frac{A_c}{A_s}\\]\nTo approximate the ratio \\(\\frac{A_c}{A_s}\\) we will generate a (uniform) pseudo-random number between 0 and 1 for our x-coordinates, and a (uniform) pseudo-random number between 0 and 1 for our y-coordinates. We will then check if our random point lies in or out of the circle. The probability \\(P_i\\) that our point lies in the circle is related to the area ratio and so value of pi:\n\\[P_i = \\frac{A_c}{A_s} = \\frac{\\pi}{4}\\]\n\n\nThe Monte Carlo area method can be translated into Python code\n\n# importing the modules we will need\nimport random\nimport math\n\n# in this function we generate random numbers and count how many lie within the circle\ndef estimate_pi(num_points):\n    \n    points = []\n    hits = 0\n    for i in range(num_points):\n        # random.random returns a random number drawn from a uniform distribution from 0 to 1\n        x, y = random.random(), random.random()\n        # we test if the point is within the circle (using the equation for a circle, X^2+y^2=r^2)\n        if x*x + y*y &lt; 1.0:\n            hits += 1\n    \n    probability = hits / num_points\n    return probability*4\n        \n\n\nestimate_pi(1000)\n\n3.104\n\n\n\nestimate_pi(2000)\n\n3.176\n\n\nThis method usually improves with the number of points, however there can be some variation due to the randomness of the numbers used. If you would like others to reproduce your exact results, you can seed the (pseudo-)random number generator:\n\nrandom.seed(1)\nprint(\"error is {}\".format(math.pi-estimate_pi(1000)))\n\nerror is 0.029592653589793017\n\n\n\nrandom.seed(1)\nprint(\"error is {}\".format(math.pi-estimate_pi(2000)))\n\nerror is -0.004407346410206792\n\n\n\n\nMonte Carlo integration is computationally efficient for particular types of integrand\nThe error when using Monte Carlo integration is proportional to \\(N^{-\\frac{1}{2}}\\), which is larger than the rectangular slice approach with error order \\(h \\propto N^{-1}\\) (where \\(N\\) in this case is the number of integration slices). However Monte Carlo methods are more flexible and can be used where other methods break-down: for example, they are particularly useful for integrating functions where the integrand varies very quickly, and/or where the integral is over many variables. In many cases, for “well behaved” functions, an approach based on Riemann summation will give more accurate and computationally efficient results.\n\n\n\n\n\n\nKeypoints\n\n\n\n\nThere are many different numerical methods for calculating integrals\nMonte Carlo methods calculate the answers to exact calculations by doing random calculations\nThe Monte Carlo “area method” estimates integrals by generating a uniform sample of points and counting how many fall into a planar region\nWe can use the Monte Carlo area method to estimate pi\nThe Monte Carlo area method can be translated into Python code\nMonte Carlo integration is computationally efficient for particular types of integrand\n\n\n\n\n\nTest your understanding\n\n\n\n\n\n\nIntegrating a semicircle re-visited\n\n\n\n\n\n\nUse Monte Carlo integration (with 100 random points) to calculate the value of the integral:\n\n\\[ I = \\int_{-1}^1\\sqrt{1-x^2}\\mathrm{d}x \\]\n\nHow does this compare to exact answer? (Hint: the integrand is a semicircle of radius 1)\nHow can you improve the accuracy of your estimate?\nIncrease the number of points until you get an accuracy comparable (same order of magnitude) as the Riemann sum method with 100 points.\nUse the %%timeit notebook magic to compare the calculation times for the Riemann sum method and Monte Carlo method. Which is more efficient?\n\n\n\n\n\n\n\nShow answer\n\n\n\n\n\n\nWe want to calculate the area of a semicircle with radius 1. We can adapt the approach used in the Monte Carlo tutorial but, in this case, we use the fact that \\(P_i = \\frac{A_c}{A_r}\\) where \\(A_r\\) is a rectangle of length 2 (as the semicircle goes from \\(-1\\) to \\(1\\)) and height 1.\n\nimport random\nimport math\n    \ndef estimate_semicircle_area(num_points):\n    \n    points = []\n    hits = 0\n    for i in range(num_points):\n        # random.uniform(a,b) returns a random number drawn from a uniform distribution from a to b\n        x, y = random.uniform(-1,1), random.uniform(0,1)\n        # we test if the point is within the circle (using the equation for a circle, X^2+y^2=r^2)\n        if x*x + y*y &lt; 1.0:\n            hits += 1\n    \n    probability = hits / num_points\n    rectangle_area = 2\n    return probability*rectangle_area\n    \nestimate_semicircle_area(100)\n1.58\nNote that your estimate will be different as you be using a different set of random numbers.\n\nThe exact answer is \\(\\frac{\\pi}{2}\\). The error on our calculation is\n\nmath.pi/2 - estimate_semicircle_area(100)\n-0.04920367320510355\n\nTo improve the accuracy we can use a larger number of random points:\n\nmath.pi/2 - estimate_semicircle_area(1000)\n0.04679632679489654\nmath.pi/2 - estimate_semicircle_area(10000)\n-0.0048036732051033315\n\nIncreasing the number of points to 10,000 gives an error comparable to the Riemann sum method with 100 integration slices (where the error is 0.002).\nLet’s use the %%timeit magic to time how long each takes to run\n\n%%timeit\nestimate_semicircle_area(10000)\n6.23 ms ± 370 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n%%timeit\nrectangular_slice_integral(semicircle, -1, 1, 100)\n31 µs ± 1.09 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\nAs each method gives roughly the same accuracy, but the estimate_semicircle_area is 100x smaller, we can deduce that the Monte Carlo method implemented in estimate_semicircle_area is considerably less efficient than the Riemann summation method implemented in rectangular_slice_integral. However the Monte Carlo method is useful for badly behaving systems, as we will see in the lesson exercises."
  },
  {
    "objectID": "notebooks/error_accuracy_speed.html",
    "href": "notebooks/error_accuracy_speed.html",
    "title": "Evaluating numerical error, accuracy and speed",
    "section": "",
    "text": "Questions\n\n\n\n\nWhich numerical errors are unavoidable in a Python programme?\nHow do I choose the optimum step size \\(h\\) when using the finite difference method?\nHow can I measure the speed of my code?\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\nUnderstand that there are unavoidable rounding errors when working with floats\nWrite code for testing if two floats are equivalent (to within machine accuracy)\nCalculate the optimum step size \\(h\\) for finite difference methods\nMeasure the length of time a Notebook cell takes to run using the %time magic.\n\n\n\n\nFinite difference methods have two sources of error\nThe error in a method’s solution is defined as the difference between the approximation and the exact analytical solution. The two sources of error in finite difference methods are round-off error, the loss of precision due to computer rounding of decimal quantities, and truncation error or discretization error, the difference between the exact solution of the original differential equation and the exact quantity assuming perfect arithmetic (that is, assuming no round-off).\n\n\nComputers have inherent limitations that lead to rounding errors\nWe have seen how computer programming can be used to model physical systems. However computers have inherent limitations - they cannot store real numbers with an infinite number of decimal places.\nIn many cases this is not a problem, but it is something to be aware of. For example, take the following piece of code:\n\ndef add_numbers(a,b):\n    return a+b\n\ndef test_add_numbers():\n    assert add_numbers(0.1,0.2) == 0.3\n\n\nadd_numbers is a function for adding two Python objects a and b.\ntest_add_numbers is a function for testing is the add_numbers function is working as expected (we will see more on testing later in the course). This function will raise an error if \\(0.1 + 0.2\\) does not equal 0.3.\n\n\nadd_numbers(1,2)\n\n3\n\n\nThe add_numbers function works as expected if we pass it two integers. However when we run the test function we raise an assertion error:\n\ntest_add_numbers()\n\nAssertionError: \n\n\nThis  rounding error  is given because \\(0.1+0.2\\) does not equal 0.3 exactly:\n\n0.1+0.2\n\n0.30000000000000004\n\n\nThis is because floating point numbers (floats) are represented on the computer to a certain precision. In Python the standard level of precision is 16 significant digits.\n\nNote: The largest value you can give a floating point variable is about \\(10^{308}\\). The smallest is -\\(10^{308}\\). If you exceed these values (which is unlikely) then the computer will return an Overflow error. In contrast, PYthon can represent integers to any precision - limited only by the memory of the machine.\n\n\n\nDo not test for the equality of two floats\nAs we have seen in the previous example, we should not test for the equality of two floats. Instead we should ask if they are equal up to a given precision:\n\ndef add_numbers(a,b):\n    return a+b\n\nepsilon = 1e-12\n\ndef test_add_numbers():\n    assert abs(add_numbers(0.1,0.2) - 0.3) &lt; epsilon\n\n\ntest_add_numbers()\n\n\n\nThe finite difference discretisation error is associated with step size \\(h\\)\nThe finite difference method is only exact in the limit that \\(h\\) is zero. This is not possible, so the differences we calculate do not give exact derivatives. However one way of improving the finite-\\(h\\) approximation is to decrease the step size in space (use a higher number of points on our real space grid). To demonstrate this, consider the Taylor expansion of \\(f(x)\\) about \\(x\\):\n\\[\\begin{equation}\nf(x+h) = f(x) + hf'(x) +\\frac{1}{2}h^2f''(x) + \\ldots\n\\end{equation}\\]\nRe-arranging gives an expression for the forward difference method:\n\\[\\begin{equation}\nf'(x) = \\frac{f(x+h) - f(x)}{h} - \\frac{1}{2}hf''(x) + \\ldots\n\\end{equation}\\]\nWhen we calculate the forward difference method we ignore all terms \\(f''(x)\\) and higher. The size of the neglected terms gives the approximation error. We can see from above that the error us linear in \\(h\\) so, as we would expect, the approximation generally improves as we decrease step size \\(h\\). Note however that when the step size is decreased the programme will run more slowly.\n\n\nIt is possible to make the step size \\(h\\) too small\nWe also need to think about the rounding errors associated with finite differences. Counter-intuitively, these errors can increase as we decrease the step size \\(h\\).\nA computer can typically store a number \\(f(x)\\) to an accuracy of 16 significant figures, or \\(Cf(x)\\) where \\(C=10^{-16}\\). In the worst case, this makes the error \\(\\epsilon\\) on our derivative:\n\\[\\begin{equation}\n\\epsilon = \\frac{2C|f(x)|}{h} + \\frac{1}{2}h|f''(x)|.\n\\end{equation}\\]\nWe want to find the value of \\(h\\) which minimises this error so we differentiate with respect to \\(h\\) and set the result equal to zero.\n\\[\\begin{equation}\n-\\frac{2C|f(x)|}{h^2} + h|f''(x)|\n\\end{equation}\\]\n\\[\\begin{equation}\nh = \\sqrt{4C\\lvert\\frac{f(x)}{f''(x)}\\rvert}\n\\end{equation}\\]\nIf \\(f(x)\\) and \\(f''(x)\\) are order 1, then \\(h\\) should be order \\(\\sqrt{C}\\), or \\(10^{-8}\\).\nSimilar reasoning applied to the central difference formula suggests that the optimum step size for this method is \\(10^{-5}\\).\n\n\nUse the %time magic to measure the length of time a Jupyter Notebook cell takes to run\nIt is often possible to use a range of numerical methods to achieve the same level of accuracy. In this case, we may want to consider code speed - which method will run the fastest? This is a particularly important question when writing code that is computationally intensive. To measure the length of time it takes for a Jupyter Notebook cell to run we can use the % time magic\n\ndef sum_integers(max_integer):\n    count = 0\n    for i in range(max_integer):\n        count += max_integer + 1\n        \n    return count\n        \n\n\n%time sum = sum_integers(1000000)\n\nCPU times: user 72.3 ms, sys: 2.91 ms, total: 75.2 ms\nWall time: 73.8 ms\n\n\n\n\n\n\n\n\nKeypoints\n\n\n\n\nFinite difference methods have two sources of error\nComputers have inherent limitations that lead to rounding errors\nDo not test for the equality of two floats\nThe finite difference discretisation error is associated with step size ℎ\nIt is possible to make the step size ℎ too smal\nUse the %time magic to measure the length of time a Jupyter Notebook cell takes to run"
  },
  {
    "objectID": "notebooks/ODE_boundary_and_initial.html",
    "href": "notebooks/ODE_boundary_and_initial.html",
    "title": "Initial and boundary conditions",
    "section": "",
    "text": "Questions\n\n\n\n\nHow can I describe radioactive decay using a first-order ODE?\nWhat are initial conditions and why are they important?\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\nMap between physical notation for a particular problem and the more general notation for all differential equations\nSolve a linear, first order, separable ODE using integration\nUnderstand the physical importance of initial conditions\n\n\n\n\nRadioactive decay can be modelled a linear, first-order ODE\nAs our first example of an ODE we will model radioactive decay using a differential equation.\nWe know that the decay rate is proportional to the number of atoms present. Mathematically, this relationship can be expressed as:\n\\[\\begin{equation}\n\\frac{d N}{d t} = -\\lambda N\n\\end{equation}\\]\nNote that we could choose to use different variables, for example:\n\\[\\begin{equation}\n\\frac{d y}{d x} = cy\n\\end{equation}\\]\nHowever we try to use variables connected to the context of the problem. For example \\(N\\) for the Number of atoms.\nFor example, if we know that 10% of atoms will decay per second we could write:\n\\[\\begin{equation}\n\\frac{d N}{d t} = -0.1 N\n\\end{equation}\\]\nwhere \\(N\\) is the number of atoms and \\(t\\) is time measured in seconds.\nThis equation is linear and first-order.\n\n\n\nphysical notation\ngeneric notation\n\n\n\n\nnumber of atoms \\(N\\)\ndependent variable \\(y\\)\n\n\ntime \\(t\\)\nindependent variable \\(x\\)\n\n\ndecay rate \\(\\frac{dN}{dt}\\)\ndifferential \\(\\frac{dy}{dx}\\)\n\n\nconstant of proportionality $ $\nparameter \\(c\\)\n\n\n\n\n\nThe equation for radioactive decay is separable and has an analytic solution\nThe radioactive decay equation is separable. For example,\n\\[\\begin{equation}\n\\frac{d N}{d t} = -\\lambda N\n\\end{equation}\\]\nCan be seperated as\n\\[\\begin{equation}\n\\frac{dN}{N} = -\\lambda dt.\n\\end{equation}\\]\nWe can then integrate each side:\n\\[\\begin{equation}\n\\ln N = -\\lambda t + const.\n\\end{equation}\\]\nand solve for N:\n\\[\\begin{equation}\nN = e^{-\\lambda t}e^{\\textrm{const.}}\n\\end{equation}\\]\n\n\n\n\n\n\nNote\n\n\n\nRemember that \\(\\int \\frac{1}{x} dx = \\ln x + \\textrm{const.}\\)\n\n\n\n\nTo model a physical system an initial value has to be provided\nAt the beginning (when \\(t=0\\)):\n\\[\\begin{equation}\nN = e^{-\\lambda t}e^{\\textrm{const.}} = e^{0}e^{\\textrm{const.}} = e^{\\textrm{const.}}\n\\end{equation}\\]\nSo we can identify \\(e^{\\textrm{const.}}\\) as the amount of radioactive material that was present in the beginning. We denote this starting amount as \\(N_0\\).\nSubstituting this back into Equation 4, the final solution can be more meaningfully written as:\n\\[\\begin{equation}\nN = N_0 e^{-\\lambda t}\n\\end{equation}\\]\nWe now have not just one solution, but a whole class of solutions that are dependent on the initial amount of radioactive material \\(N_0\\).\nRemember that not all mathematical solutions make physical sense. To model a physical system, this initial value (also known as initial condition) has to be provided alongside the constant of proportionality \\(\\lambda\\).\n\n\nODEs can have initial values or boundary values\nODEs have either initial values or boundary values. For example, using Newton’s second laws we could calculate the distance \\(x\\) an object travels under the influence of gravity over time \\(t\\)\n\\[\\begin{equation}\n\\frac{\\mathrm{d}^2x}{\\mathrm{d}t^2} = -g\n\\end{equation}\\]\nAn initial value problem would be where we know the starting position and velocity. A boundary value problem would be where we specify the position of the ball at times \\(t=t_0\\) and \\(t=t_1\\).\nIn this course we will only study ODEs with initial values. ODEs with boundary values are more difficult to solve, but you can find additional materials in the course textbook.\n\n\nThe number of initial conditions depends on the order of the differential equation\nOur radioactive decay example is a first-order ODE and so we only had to provide a single initial condition. For second-order ODEs (such as acceleration under gravity) we need to provide two initial/boundary conditions, for third-order ODEs we would need to provide three, and so on.\n\n\n\n\n\n\nKeypoints\n\n\n\n\nRadioactive decay can be modelled a linear, first-order ODE\nThe equation for radioactive decay is separable and has an analytic solution\nTo model a physical system an initial value has to be provided\nThe number of initial conditions depends on the order of the differential equation\n\n\n\n\n\nTest your understanding\n\n\n\n\n\n\nTerminology tables\n\n\n\n\n\nIn this tutorial there is a table listing the dependent variable, independent variable, differential and parameter(s). Complete a similar table for the following physical systems:\n\n\n\nphysical notation\ngeneric notation\n\n\n\n\n\ndependent variable(s)\n\n\n\nindependent variable(s)\n\n\n\ndifferential(s)\n\n\n\nparameter(s)/constants(s)\n\n\n\n\nThe amount of charge that flows every second from a capacitor,\n\n\\[\\begin{equation}\n\\frac{\\mathrm{d} Q}{\\mathrm{d} t} = -\\beta Q\n\\end{equation}\\]\n\nThe motion of a non-linear pendulum driven by a force oscillating at frequency \\(\\sigma\\),\n\n\\[\\begin{equation}\n\\frac{\\mathrm{d}^2\\theta}{\\mathrm{t}^2} = -\\frac{g}{l}\\sin(\\theta) + C\\cos(\\theta)\\sin(\\sigma t).\n\\end{equation}\\]\n\n\n\n\n\n\nShow answer\n\n\n\n\n\n\n\n\n\n\n\nphysical notation\ngeneric notation\n\n\n\n\n\\(Q\\)\ndependent variable(s)\n\n\n\\(t\\)\nindependent variable(s)\n\n\n\\(\\frac{\\mathrm{d}Q}{\\mathrm{d}t}\\)\ndifferential(s)\n\n\n\\(\\beta\\)\nparameter(s)/constant(s)\n\n\n\n\n\n\n\nphysical notation\ngeneric notation\n\n\n\n\n\\(\\theta\\)\ndependent variable(s)\n\n\n\\(t\\)\nindependent variable(s)\n\n\n\\(\\frac{\\mathrm{d}^2\\theta}{\\mathrm{d} t^2}\\)\ndifferential(s)\n\n\n\\(l\\), \\(C\\), \\(\\sigma\\), \\(g\\)\nparameter(s)/constant(s)"
  },
  {
    "objectID": "notebooks/PDE_classification.html",
    "href": "notebooks/PDE_classification.html",
    "title": "Classification",
    "section": "",
    "text": "Questions\n\n\n\n\nWhich physical systems can be described using a PDE?\nWhat is the Laplacian operator?\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\nRecognise common classes of PDE: the diffusion equation, Poisson’s equation and the wave equation\nExpress the Laplacian as a differential operator\n\n\n\n\nPartial differential equations have multiple inputs\nIn the previous section of the course we studied ordinary differential equations. ODEs have a single input (also known as independent variable) - for example, time.\nPartial differential equations (PDEs) have multiple inputs (independent variables). For example, think about a sheet of metal that has been heated unevenly across the surface. Over time, heat will diffuse through the 2-dimensional sheet. The temperature depends on both time and position - there are two inputs.\nBecause PDEs have multiple inputs they are generally much more difficult to solve analytically than ODEs. However, there are a range of numerical methods that can be used to find approximate solutions.\n\n\nPDEs have application across a wide variety of topics\nThe same type of PDE often appears in different contexts. For example, the diffusion equation takes the form:\n\\[\\begin{equation}\n\\nabla^2T = \\alpha \\frac{\\partial T}{\\partial t}\n\\end{equation}\\]\nWhen used to describe heat diffusion, this PDE is known as the heat equation. This same PDE however can be used to model other seemingly unrelated processes such as brownian motion, or used in financial modelling via the Black-Sholes equation.\nAnother type of PDE is known as Poisson’s equation:\n\\[\\begin{equation}\n\\nabla^2\\phi = f(x,y,z)\n\\end{equation}\\]\nPoisson’s equation can be used to describe electrostatic forces, where \\(\\phi\\) is the electric potential. It can also be applied to mechanics (where \\(\\phi\\) is the gravitational potential) or thermodynamics (where \\(\\phi\\) is the temperature). When \\(f(x,y,z)=0\\) this equation is known as Laplace’s equation.\nThe third common type of PDE is the wave equation:\n\\[\\begin{equation}\n\\nabla^2r = \\alpha \\frac{\\partial^2 r}{\\partial t^2}\n\\end{equation}\\]\nThis describes mechanical processes such as the vibration of a string or the motion of a pendulum. It can also be used in electrodynamics to describe the exchange of energy between the electric and magnetic fields.\nIn this course we will look at techniques for solving the diffusion equation and Poisson’s equation, but many of the topics we will discuss - such as boundary conditions, and finite difference methods - can be transferred to PDEs more generally.\n\n\nThe Laplacian operator corresponds to an average rate of change\nBut what is the operator \\(\\nabla^2\\)?. This is the Laplacian operator. When applied to \\(\\phi\\) and written in full for a three dimensional cartesian coordinate system with dependent variables \\(x\\), \\(y\\) and \\(z\\) it takes the following form:\n\\[\\begin{equation}\n\\nabla^2\\phi = \\frac{\\partial^2\\phi}{\\partial x^2} + \\frac{\\partial^2\\phi}{\\partial y^2} + \\frac{\\partial^2\\phi}{\\partial z^2}.\n\\end{equation}\\]\nWe can think of the laplacian as encoding an average rate of change. To develop an intuition for how the laplacian can be interpreted physically, we need to understand two related operators - div and curl. We will not explore these operators further in this lesson, but a video is included under external resources.\n\n\n\n\n\n\nKeypoints\n\n\n\n\nPDEs have multiple inputs (independent variables)\nPDEs have application across a wide variety of topics\nThe Laplacian operator corresponds to an average rate of change\n\n\n\n\n\nTest your understanding\n\n\n\n\n\n\nClassifying PDEs\n\n\n\n\n\nDecide whether the following systems are a boundary value problem or initial value problem, and whether they are diffusion-like, wave-like or poisson-like:\n\nBrownian motion of small particles in a liquid (the random-walk)\n\n\\[\\begin{equation}\n\\frac{dP}{dt} = \\frac{l^2}{2Np^2}\\nabla^2P\n\\end{equation}\\]\n\nthe Klein-Gordon equation for describing the energy-momentum relation of relativistic particles:\n\n\\[\\begin{equation}\n\\left(\\frac{1}{c^2}\\frac{\\partial^2}{\\partial t^2} - \\nabla^2 + \\frac{m^2c^2}{\\hbar^2}\\right)\\phi(t,x)=0\n\\end{equation}\\]\n\n\n\n\n\n\nShow answer\n\n\n\n\n\n\nthis is an initial value problem described by the diffusion equation\n\nthis is an initial value problem described by the wave equation"
  }
]